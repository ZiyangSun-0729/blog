# 第一章：k8s介绍及前期环境部署

​                                                  ![img](https://cdn.nlark.com/yuque/0/2024/png/44499768/1715757186288-e5ae30be-603f-4a7c-9bc8-fa218a40120c.png)

kubernetes（k8s）是2014年由Google公司基于Go语言编写的一款开源的容器集群编排系统，用于自动化容器的部署、扩缩容和管理；

kubernetes（k8s）是基于Google内部的Borg系统的特征开发的一个版本，集成了Borg系统大部分优势；

官方地址：https://Kubernetes.io

代码托管平台：https://github.com/Kubernetes



**除了k8s还有哪些容器编排系统？**如：`docker swarm`、`Openshift`、`Rancher`、`Mesos`等。

## kubernetes具备的功能

- **自我修复：**k8s可以监控容器的运行状况，并在发现容器出现异常时自动重启故障实例；
- **弹性伸缩**：k8s可以根据资源的使用情况自动地调整容器的副本数。例如，在高峰时段，k8s可以自动增加容器的副本数以应对更多的流量；而在低峰时段，k8s可以减少应用的副本数，节省资源；
- **资源限额：**k8s允许指定每个容器所需的CPU和内存资源，能够更好的管理容器的资源使用量；
- **滚动升级：**k8s可以在不中断服务的情况下滚动升级应用版本，确保在整个过程中仍有足够的实例在提供服务；
- **负载均衡：**k8s可以根据应用的负载情况自动分配流量，确保各个实例之间的负载均衡，避免某些实例过载导致的性能下降；
- **服务发现：**k8s可以自动发现应用的实例，并为它们分配一个统一的访问地址。这样，用户只需要知道这个统一的地址，就可以访问到应用的任意实例，而无需关心具体的实例信息；
- **存储管理：**k8s可以自动管理应用的存储资源，为应用提供持久化的数据存储。这样，在应用实例发生变化时，用户数据仍能保持一致，确保数据的持久性；
- **密钥与配置管理：**Kubernetes 允许你存储和管理敏感信息，例如：密码、令牌、证书、ssh密钥等信息进行统一管理，并共享给多个容器复用；

## kubernetes集群角色

k8s集群需要建⽴在多个节点上，将多个节点组建成一个集群，然后进⾏统⼀管理，但是在k8s集群内部，这些节点⼜被划分成了两类⻆⾊：

- **Master管理节点：**负责集群的所有管理工作，和协调集群中运行的容器应用； 
- **Node工作节点：**负责运行集群中所有用户的容器应用， 执行实际的工作负载 ； 

**Master管理节点组件：**

- **API Server：**作为集群的控制中心，处理外部和内部通信，接收用户请求并处理集群内部组件之间的通信；
- **Scheduler：**负责将待部署的 Pods 分配到合适的 Node 节点上，根据资源需求、策略和约束等因素进行调度；
- **Controller Manager：**管理集群中的各种控制器，例如： Deployment、ReplicaSet、StatefulSet控制器等，来管理集群中的各种资源；
- **etcd：**作为集群的数据存储，保存集群的配置信息和状态信息；

**Node工作节点组件：**

- **Kubelet：**负责与 Master 节点通信，并根据 Master 节点的调度决策来创建、更新和删除 Pod，同时维护 Node 节点上的容器状态；
- **容器运行时**（如 Docker、containerd 等）：负责运行和管理容器，提供容器生命周期管理功能。例如：创建、更新、删除容器等；
- **Kube-proxy：**负责为集群内的服务实现网络代理和负载均衡，确保服务的访问性；

**非必须的集群组件：**

- **DNS服务：**严格意义上的必须插件，在k8s中，很多功能都需要用到DNS服务，例如：服务发现、有状态应用的访问等；
- **Dashboard：** 是k8s集群WEB管理界面，如：Rancher、Kuboard等
- **资源监控**：例如metrics-server监视器，用于监控集群中资源利用率；

## kubernetes集群类型

- **一主多从集群：**由一台Master管理节点和多台Node工作节点组成，生产环境下Master节点存在单点故障的风险，适合学习和测试环境使用；
- **多主多从集群：**由多台Master管理节点和多Node工作节点组成，安全性高，适合生产环境使用；

## kubernetes集群规划

| **IP地址**    | **主机名称** | **主机角色**   | **操作系统** | **主机配置** |
| ------------- | ------------ | -------------- | ------------ | ------------ |
| 192.168.0.111 | master01     | 管理节点       | Rocky 9.0    | 2C/4G/50G    |
| 192.168.0.112 | master02     | 管理节点       | Rocky 9.0    | 2C/4G/50G    |
| 192.168.0.113 | master03     | 管理节点       | Rocky 9.0    | 2C/2G/50G    |
| 192.168.0.114 | node01       | 工作节点       | Rocky 9.0    | 2C/4G/50G    |
| 192.168.0.115 | node02       | 工作节点       | Rocky 9.0    | 2C/4G/50G    |
| 192.168.0.117 | k8s-ha1      | 主负载均衡节点 | Rocky 9.0    | 2C/4G/50G    |
| 192.168.0.118 | k8s-ha2      | 备负载均衡节点 | Rocky 9.0    | 2C/4G/50G    |

## kubernetes集群环境准备

**以下前期环境准备需要在所有节点都执行（**不包括负载均衡节点**）**

**提示：不要忘了在每个节点都跑一遍环境初始化脚本！！！！！**



**配置集群之间本地解析**

集群在初始化时，需要解析每个节点主机名，作为该节点在集群中的名称

```shell
192.168.0.111 master01
192.168.0.112 master02
192.168.0.113 master03
192.168.0.114 node01
192.168.0.115 node02
```

## **开启bridge网桥过滤**

bridge(桥接) 是 Linux 系统中的一种虚拟网络设备，它充当一个虚拟的交换机，为集群内的容器提供网络通信功能，容器就可以通过这个 bridge 与其他容器或外部网络通信了。

`/etc/sysctl.d/`目录用于存放配置内核参数的文件，文件以`.conf`结尾，这些文件会在系统启动时自动加载。

```shell
cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
```

<details class="lake-collapse"><summary id="u624dee65"><strong><span class="ne-text">参数解释：</span></strong></summary><p id="ubedf7fc8" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">net.bridge.bridge-nf-call-ip6tables = 1 </span><span class="ne-text" style="background-color: #FBE4E7"> //对网桥上的IPv6数据包通过iptables处理</span></p><p id="u6be1b54b" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">net.bridge.bridge-nf-call-iptables = 1  </span><span class="ne-text" style="background-color: #FBE4E7"> //对网桥上的IPv4数据包通过iptables处理</span></p><p id="u8563908a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">net.ipv4.ip_forward = 1                        </span><span class="ne-text" style="background-color: #FBE4E7"> //开启IPv4路由转发,来实现集群中的容器跨网络通信</span></p></details>

由于开启bridge功能，需要加载br_netfilter模块来允许在bridge设备上的数据包经过iptables防火墙处理

```shell
modprobe br_netfilter && lsmod | grep br_netfilter
```

<details class="lake-collapse"><summary id="ub7783183"><strong><span class="ne-text">命令解释：</span></strong></summary><p id="ue6d332fb" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">modprobe       	</span><span class="ne-text" style="background-color: #FBE4E7"> //命令可以加载内核模块</span></p><p id="ua66f7fff" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">br_netfilter    		</span><span class="ne-text" style="background-color: #FBE4E7">//模块模块允许在bridge设备上的数据包经过iptables防火墙处理</span></p></details>

加载配置文件，使上述配置生效

```shell
sysctl -p /etc/sysctl.d/k8s.conf
```

## 安装IPvs代理模块

在k8s中Service有两种代理模式，一种是基于iptables的，一种是基于ipvs，两者对比ipvs模式性能更高效，如果想要使用ipvs模式，需要手动载入ipvs模块。

`ipset`和`ipvsadm`是网络管理和负载均衡相关的软件包，提供`ip_vs`模块

```shell
dnf -y install ipset ipvsadm
```

将需要加载的ipvs相关模块写入到文件中

`/etc/modules-load.d/`目录主要用于在系统启动时加载用户自定义的内核模块，这个目录中的文件以 `.conf` 结尾，文件中指定需要加载的内核模块。

```shell
cat > /etc/modules-load.d/ip_vs.conf <<EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
EOF
```

<details class="lake-collapse"><summary id="u392e50f0"><strong><span class="ne-text">模块介绍：</span></strong></summary><p id="uaf4ca6c1" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ip_vs         	</span><span class="ne-text" style="background-color: #FBE4E7"> //提供负载均衡的模块,支持多种负载均衡算法,如轮询、最小连接、加权最小连接等</span></p><p id="u882c0243" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ip_vs_rr      	 </span><span class="ne-text" style="background-color: #FBE4E7">//轮询算法的模块（默认算法）</span></p><p id="udebdc2be" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ip_vs_wrr     	</span><span class="ne-text" style="background-color: #FBE4E7"> //加权轮询算法的模块,根据后端服务器的权重值转发请求</span></p><p id="u8610511e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ip_vs_sh     	</span><span class="ne-text" style="background-color: #FBE4E7"> //哈希算法的模块,同一客户端的请求始终被分发到相同的后端服务器,保证会话一致性</span></p><p id="uad109d55" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">nf_conntrack  	</span><span class="ne-text" style="color: #DF2A3F"> </span><span class="ne-text" style="background-color: #FBE4E7">//链接跟踪的模块,用于跟踪一个连接的状态,例如 TCP 握手、数据传输和连接关闭等</span></p></details>

 加载模块生效（这个文件会在下次系统启动时自动生效）

```shell
systemctl restart systemd-modules-load.service
```

过滤模块，验证是否成功加载  

```shell
lsmod | grep ip_vs
===========================================================
ip_vs_sh               16384  0
ip_vs_wrr              16384  0
ip_vs_rr               16384  0
ip_vs                 188416  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr
nf_conntrack          176128  1 ip_vs
nf_defrag_ipv6         24576  2 nf_conntrack,ip_vs
libcrc32c              16384  3 nf_conntrack,xfs,ip_vs
```

## 关闭SWAP分区

为了保证 kubelet 正常工作，k8s强制要求禁用

```shell
swapoff -a && \
sed -ri 's/.*swap.*/#&/' /etc/fstab
```

## Docker环境准备

添加阿里云docker-ce仓库

```shell
dnf install -y yum-utils && \
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
```

安装docker软件包

```shell
dnf install docker-ce-20.10.24-3.el9.x86_64 -y
```

启用Docker Cgroup用于限制进程的资源使用量，如CPU、内存资源

```shell
# step 1：创建/etc/docker目录
mkdir /etc/docker
# step 2：配置Cgroup和镜像加速器
cat > /etc/docker/daemon.json <<EOF
{
        "exec-opts": ["native.cgroupdriver=systemd"],
        "registry-mirrors": ["https://docker.1ms.run"]
}
EOF
```

启动docker并设置随机自启

```shell
systemctl enable docker --now
```

## 负载均衡环境准备

**haproxy：**为apiserver提供代理，集群的管理请求通过VIP进行接收，haproxy将所有管理请求轮询转发到每个master节点上。

**Keepalived：**为haproxy提供vip（192.168.0.100）在二个haproxy之间提供主备，实现代理故障自动切换。

![img](https://cdn.nlark.com/yuque/0/2024/png/44499768/1715770870995-4ea25c8a-8bd9-4ea5-b769-af07d0e6f34d.png)



**以下操作只需要在****k8s-ha1****、****k8s-ha2****配置。**

**提示：不要忘了跑一遍环境初始化脚本！！！！！**

1. 安装haproxy和keepalived软件包

```shell
dnf install haproxy keepalived -y 
```

1. 修改haproxy配置文件，文件内容在k8s-ha1与k8s-ha2节点一致

```shell
global
  maxconn  2000                   #单个进程最大并发连接数
  ulimit-n  16384                 #每个进程可以打开的文件数量
  log  127.0.0.1 local0 err       #日志输出配置，所有日志都记录在本机系统日志，通过 local0 输出
  stats timeout 30s               #连接socket超时时间

defaults
  log global                      #定义日志为global（全局）
  mode  http                      #使用的连接协议
  option  httplog                 #日志记录选项，httplog表示记录与HTTP会话相关的日志
  timeout connect 5000            #定义haproxy将客户端请求转发至后端服务器所等待的超时时长
  timeout client  50000           #客户端非活动状态的超时时长
  timeout server  50000           #客户端与服务器端建立连接后，等待服务器端的超时时长
  timeout http-request 15s        #客户端建立连接但不请求数据时，关闭客户端连接超时时间
  timeout http-keep-alive 15s     # session 会话保持超时时间

frontend monitor-in               #监控haproxy服务本身
  bind *:33305                    #监听的端口
  mode http                       #使用的连接协议
  option httplog                  #日志记录选项，httplog表示记录与HTTP会话相关的日志
  monitor-uri /monitor            #监控URL路径

frontend k8s-master               #接收请求的前端名称，名称自定义，类似于Nginx的一个虚拟主机server。
  bind 0.0.0.0:6443               #监听客户端请求的 IP地址和端口（以包含虚拟IP）
  bind 127.0.0.1:6443 
  mode tcp                        #使用的连接协议
  option tcplog                   #日志记录选项，tcplog表示记录与tcp会话相关的日志
  tcp-request inspect-delay 5s    #等待数据传输的最大超时时间
  default_backend k8s-master      #将监听到的客户端请求转发到指定的后端

backend k8s-master                #后端服务器组，要与前端中设置的后端名称一致
  mode tcp                        #使用的连接协议
  option tcplog                   #日志记录选项，tcplog表示记录与tcp会话相关的日志
  option tcp-check                #tcp健康检查
  balance roundrobin              #负载均衡方式为轮询
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server master01   192.168.0.111:6443  check  # 根据自己环境修改后端实例IP
  server master02   192.168.0.112:6443  check  # 根据自己环境修改后端实例IP
  server master03   192.168.0.113:6443  check  # 根据自己环境修改后端实例IP
```

k8s-ha1与k8s-ha2启动haproxy

```shell
systemctl enable haproxy --now
```

1. **修**改keepalived配置文件

k8s-ha1节点keepalived配置文件内容如下

```shell
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
//自定义健康检查脚本，健康检查名称为chk_haproxy
vrrp_script chk_haproxy {
    //健康检查的脚本路径及名称
    script "/etc/keepalived/check_haproxy.sh"
    interval 5
    weight -5
    fall 2
    rise 1
}
vrrp_instance VI_1 {
    state MASTER             //节点身份，Master节点负责处理请求并优先拥有虚拟IP地址
    interface ens32          //虚拟IP地址绑定的网卡名称，根据实际情况修改
    virtual_router_id 51 
    priority 101             //节点的优先级，决定哪个节点将成为Master，数字越大优先级越高
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass abc123
    }
    virtual_ipaddress {
        192.168.0.100/24     //定义虚拟IP地址
    }
    //引用健康检查，名称与上方vrrp_script中定义的名称保持一致
    track_script {
       chk_haproxy
    }
}
```

<details class="lake-collapse"><summary id="ua3abf131"><strong><span class="ne-text" style="color: rgb(51, 51, 51); font-size: 24px">keepalived配置文件详解</span></strong></summary><p id="ucaf60db2" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="color: #DF2A3F">//自定义健康检查脚本，健康检查名称为chk_haproxy</span></p><p id="ucf9de2ce" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">vrrp_script chk_apiserver {</span></p><p id="u4bda467d" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">   </span><span class="ne-text" style="color: #117CEE"> </span><span class="ne-text" style="color: #DF2A3F">//脚本所在的路径及名称</span></p><p id="u2e42a0fc" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    script "/etc/keepalived/check_haproxy.sh"</span></p><p id="u94becdda" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">   </span><span class="ne-text" style="color: #DF2A3F"> //监控检查的时间间隔，单位秒</span></p><p id="u25c58c2f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    interval 5</span></p><p id="u8f2d31fd" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">   </span><span class="ne-text" style="color: #DF2A3F"> //健康检查的次数，连续2次健康检查失败，服务器将被标记为不健康</span></p><p id="u0dee979a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    fall 2</span><span class="ne-text" style="color: #DF2A3F"></span></p><p id="u9c2e8f03" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">   </span><span class="ne-text" style="color: #DF2A3F"> //连续健康检查成功的次数，有1次健康检查成功，服务器将被标记为健康</span></p><p id="u5a440cf4" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    rise 1</span></p><p id="ueb2e870b" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">}</span></p><p id="u2bdbad5f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="color: #DF2A3F">//配置了一个名为VI_1的VRRP实例组</span></p><p id="ue23ea5f6" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">vrrp_instance VI_1 {</span></p><p id="ue15018ee" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">   </span><span class="ne-text" style="color: #DF2A3F"> //该节点在VRRP组中的身份，Master节点负责处理请求并拥有虚拟IP地址</span></p><p id="ub3702fc3" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    state MASTER</span></p><p id="u998d76df" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">   </span><span class="ne-text" style="color: #DF2A3F"> //实例绑定的网络接口，实例通过这个网络接口与其他VRRP节点通信，以及虚拟IP地址的绑定</span></p><p id="u6f5e273a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    interface ens32</span></p><p id="ud34eb3fe" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">   </span><span class="ne-text" style="color: #DF2A3F"> //虚拟的路由ID，范围1到255之间的整数，用于在一个网络中区分不同的VRRP实例组，但是在同一个VRRP组中的节点，该ID要保持一致</span></p><p id="u10eb5dac" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    virtual_router_id 51</span></p><p id="uc5db046a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">  </span><span class="ne-text" style="color: #DF2A3F">  //实例的优先级，范围1到254之间的整数，用于决定在同一个VRRP组中哪个节点将成为Master节点，数字越大优先级越高</span></p><p id="u815f0553" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    priority 101</span></p><p id="ueef2a602" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    </span><span class="ne-text" style="color: #DF2A3F">//Master节点广播VRRP报文的时间间隔，用于通知其他Backup节点Master节点的存在和状态，在同一个VRRP组中，所有&gt;节点的advert_int参数值必须相同</span></p><p id="u24955450" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    advert_int 2</span></p><p id="u8f37bafd" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">   </span><span class="ne-text" style="color: #DF2A3F"> //实例之间通信的身份验证机制</span></p><p id="u18512008" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    authentication {</span></p><p id="u643a56db" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">       </span><span class="ne-text" style="color: #DF2A3F"> //PASS为密码验证</span></p><p id="u4f09851b" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">        auth_type PASS</span></p><p id="uedd42c61" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">	</span><span class="ne-text" style="color: #DF2A3F">//此密码必须为1到8个字符，在同一个VRRP组中，所有节点必须使用相同的密码，以确保正确的身份验证和通信</span></p><p id="ub2c0b173" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">        auth_pass abc123</span></p><p id="u610409ea" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    }</span></p><p id="u4d5cc57f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">  </span><span class="ne-text" style="color: #DF2A3F">  //定义虚拟IP地址</span></p><p id="u59ed469f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    virtual_ipaddress {</span></p><p id="u26ed3196" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">        192.168.0.100/24      </span></p><p id="u52327f04" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    }</span></p><p id="uc7ffb83a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">  </span><span class="ne-text" style="color: #DF2A3F">  //引用自定义脚本，名称与上方vrrp_script中定义的名称保持一致</span></p><p id="u0dc980e4" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    track_script {</span></p><p id="u42552fbe" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">       chk_haproxy</span></p><p id="u5f3c4865" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">    }</span></p><p id="u8d1abe44" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">}</span></p></details>

k8s-ha2节点keepalived配置文件内容如下

```shell
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
script_user root
    enable_script_security
}
//自定义健康检查脚本，健康检查名称为chk_haproxy
vrrp_script chk_haproxy {
    //健康检查的脚本路径及名称
    script "/etc/keepalived/check_haproxy.sh"
    interval 5
    weight -5
    fall 2
    rise 1
}
vrrp_instance VI_1 {
    state BACKUP             //节点身份，BACKUP为备用节点
    interface ens32          //虚拟IP地址绑定的网卡名称，根据实际情况修改
    virtual_router_id 51 
    priority 99              //节点的优先级，BACKUP要低于Master
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass abc123
    }
    virtual_ipaddress {
        192.168.0.100/24     //定义虚拟IP地址
    }
    //引用健康检查，名称与上方vrrp_script中定义的名称保持一致
    track_script {
       chk_haproxy
    }
}
```

k8s-ha1与k8s-ha2定义检测haproxy脚本

```shell
#!/bin/bash
#检测haproxy状态
count=$(ps -C haproxy | grep -v PID | wc -l)
if [ $count -eq 0 ];then
   systemctl stop keepalived
fi
```

脚本添加执行权限

```shell
chmod +x /etc/keepalived/check_haproxy.sh
```

k8s-ha1与k8s-ha2节点启动keepalived

```shell
systemctl enable keepalived --now
```

在k8s-ha1节点确认VIP地址是否生成（ifconfig命令查看不到VIP）

```shell
ip a s ens32
```

## 集群配置免密认证

在master01主机生成密钥，下发公钥到集群其他节点。方便后期拷贝证书和相关配置文件。

```shell
ssh-keygen
for ip in 192.168.0.{112..115}
do
  ssh-copy-id $ip
done
```



# 第二章：准备集群证书

在k8s中，集群的各个组件（如kubelet、kube-scheduler、kube-controller-manager等）通过HTTPS进行相互通信，以确保通信的安全性。

k8s集群内部通讯并不需要购买商业证书，基本上都是使用自签证书来实现集群内部的通信。

## SSL/TLS基本概念

HTTPS 协议，说白了就是“HTTP 协议”和“SSL/TLS 协议”的组合。

**SSL：** Secure Socket Layer（安全套接层协议）的缩写，可以在Internet上提供秘密性传输。为啥要发明 SSL 这个协议？原先互联网上使用的 HTTP 协议是明文的，内容是不加密的，这样就很可能在内容传输时被别人监听到，对于安全性要求较高的场合，必须要加密，https就是带加密的http协议。SSL协议的发明，就解决这些问题。目前SSL有1.0，2.0，3.0。

**TLS：** Transport Layer Security（传输层安全协议）相当于SSL的升级版，可以把SSL看作是windows7，而TLS看作是windows10。很多时候我们把这两者并列称呼 SSL/TLS，目前TLS有1.0，1.1，1.2，其中1.0基于SSL 3.0，修改不大。

**SSL证书：** SSL安全协议主要用来提供对用户和服务器的认证；对传送的数据进行加密；确保数据在传送中不被改变，即数据的完整性。SSL证书通过在客户端浏览器和Web服务器之间建立一条SSL安全通道，由于SSL技术已建立到所有主要的浏览器和WEB服务器程序中，因此，仅需安装服务器证书就可以激活该功能了。通过它可以激活SSL协议，实现数据信息在客户端和服务器之间的加密传输，可以防止数据信息的泄露。保证了双方传递信息的安全性，而且用户可以通过服务器证书验证他所访问的网站是否是真实可靠。

**证书相关文件**

**公钥：** 一个算法名称加上密码串，（理解成一把锁头）通过公钥对数据加密。

**私钥：** 一个算法名称加上密码串，（理解成一把钥匙）通过私钥对加密后的数据进行解密。
**密钥：** 在**非对称**加密的领域里，指的是私钥和公钥，他们总是成对出现，其主要作用是加密和解密。常用的加密强度是2048bit。
**CA：** certificate authority，专门用来给别人进行签名的单位或者机构。
**申请（签名）文件：** 包含公钥的加密方式，以及申请人的属性信息，比如域名、国家、地区等信息，然后发给CA进行签署。

**签名过程：** CA收到申请文件后，会走核实流程，确保申请人确实是证书中描述的申请人，防止别人冒充申请者申请证书，核实通过后，会用CA的私钥对申请文件进行签名，签名后的证书包含申请者的基本信息，CA的基本信息，证书的使用年限，申请人的公钥，签名用到的摘要算法，CA的签名。
**证书文件：** 经过CA签名后得到，如果是自己的私钥给自己的公钥签名，就叫自签名。

**证书文件格式**

**PEM：**是Privacy Enhanced Mail的简称，服务器证书（Apache和nginx等）的公钥和私钥文件以及自签名证书（用于测试或局域网环境）的公钥和私钥文件都可以储存为PEM格式，扩展名为` .pem .crt .cer .key`。
**DER：**是Distinguished Encoding Rules的简称，与PEM不同之处在编码格式不同，扩展名为`.der .cer`，所有类型的证书公钥和私钥文件都可以存储为DER格式。Java和Windows服务器使用DER格式证书。
**CSR**：证书签名请求CSR，是Certificate Signing Request的简称，它是向CA机构申请数字证书时使用的请求文件。当我们准备好CSR文件后就可以提交给CA机构，等待他们给我们签名，签好名后我们会收到crt文件，即证书。保留好CSR，当权威证书颁发机构颁发的证书过期的时候，还可以用同样的CSR来申请新的证书，key保持不变。

## 下载证书工具CFSSL

在master01节点创建目录用于准备集群相关证书

```shell
mkdir /root/work && cd /root/work
```

获取 CFSSL 工具自签集群所需的证书，CFSSL 工具用于生成CA证书和配对的证书密钥。

<details class="lake-collapse"><summary id="u6ed651ed"><strong><span class="ne-text">工具说明：</span></strong></summary><p id="u2c98602f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">cfssl           	</span><span class="ne-text" style="color: #DF2A3F">//管理证书的命令行工具。</span></p><p id="uc3d06769" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">cfssljson      	</span><span class="ne-text" style="color: #DF2A3F"> //当你使用cfssl工具生成新的证书和私钥时，证书的格式是JSON对象，该工具来解析这个JSON对象，将证书和私钥提取出来，分别保存为.pem文件。</span></p><p id="u0b0ba141" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">cfssl-certinfo  </span><span class="ne-text" style="color: #DF2A3F">	//这个工具可以读取PEM格式的证书，解析证书中的信息。</span></p></details>

```shell
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
```

添加执行权限并将程序移动至 `/usr/local/bin/`

```shell
chmod +x cfssl*
mv cfssl_linux-amd64 /usr/local/bin/cfssl
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo
```

## 生成自签CA根证书

需要先准备一个证书签名请求文件CSR（Certificate Signing Request）证书签名请求文件中包含了要在证书中定义的一些信息，如证书的加密算法、证书的有效期等。

<details class="lake-collapse"><summary id="u1a863e5a"><strong><span class="ne-text">参数说明：</span></strong></summary><p id="u1807855e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">CN		</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//证书的主题名称，例如，网站证书的CN通常是其域名(自签的证书，该名称自定义)。</span></p><p id="u86f09771" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">key		</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//请求者的公钥，algo生成公钥的加密算法，size公钥的长度。</span></p><p id="u8b7ebfa1" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">names	</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//这是一个列表，包含了C(国家)、ST(省份)、L(城市)、O(公司)、OU(部门)。</span></p><p id="ubdf8fa2a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">ca		</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//该证书将被标记为CA证书 (可以用来签名其他证书)，expiry定义新的CA证书的有效期(单位是小时)。</span></p></details>

```shell
cat > ca-csr.json <<"EOF"
{
  "CN": "kubernetes",
  "key": {
      "algo": "rsa",
      "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "yesir",
      "OU": "CN"
    }
  ],
  "ca": {
          "expiry": "87600h"
  }
}
EOF
```

基于`ca-csr.json` 文件生成一个新的自签根证书，也称为CA（Certificate Authority）证书，和配套的密钥。

```shell
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

<details class="lake-collapse"><summary id="u5edefc7f"><strong><span class="ne-text">命令说明：</span></strong></summary><p id="uc5eebd47" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">cfssl gencert 			</span><span class="ne-text" style="color: #DF2A3F">//用于生成新的证书和私钥。</span></p><p id="ue2aa8f10" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">-initca				</span><span class="ne-text" style="color: #DF2A3F">//指定证书签名请求文件。</span></p><p id="ua5d8cf30" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">cfssljson -bare ca		</span><span class="ne-text" style="color: #DF2A3F">//将证书和私钥提取出来，分别保存为.pem文件。</span></p></details>

生成自签CA根证书后，会生成如下证书与之配套的文件：

`ca.pem`        //这是生成的自签名证书文件,包含证书中定义的一些信息 

`ca.csr`        //这是ca证书配套的签名请求文件, 其中包含证书一些信息 

`ca-key.pem`    //这是ca证书配套的密钥文件, 验证这个证书的真实性 

## 配置CA根证书策略

需要准备一个CA证书配置文件，用于定义CA证书的策略，例如：证书的有效期、证书的用途等。

<details class="lake-collapse"><summary id="ue7a87235"><strong><span class="ne-text">参数说明：</span></strong></summary><p id="u75d63719" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">signing		</span><span class="ne-text" style="color: #DF2A3F">//定义CA的配置，default表示默认配置，expiry定义了证书有效期为10年。</span></p><p id="ua6e978ee" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">profiles		</span><span class="ne-text" style="color: #DF2A3F">//定义证书策略，策略名为（kubernetes），在该策略中通过（usages）定义了新生成的证书可以用于哪些用途，如（signing）新生成的证书可以用于数字签名、（key encipherment）密钥加密、（server auth）服务器身份认证、（client auth）和客户端身份认证。</span></p></details>

```shell
cat > ca-config.json <<"EOF"
{
  "signing": {
      "default": {
          "expiry": "87600h"
        },
      "profiles": {
          "kubernetes": {
              "usages": [
                  "signing",
                  "key encipherment",
                  "server auth",
                  "client auth"
              ],
              "expiry": "87600h"
          }
      }
  }
}
EOF
```



# 第三章：部署ETCD集群

etcd作为集群后端存储，以存储集群中的所有数据，例如：节点信息、Pod信息、配置资源、密钥资源、服务状态等， 满足Kubernetes对于数据存储的需求。

## 生成etcd相关证书

生成etcd证书签名请求文件

<details class="lake-collapse"><summary id="u45a97464"><strong><span class="ne-text">参数说明：</span></strong></summary><p id="uc68017a3" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">CN		</span><span class="ne-text" style="color: #DF2A3F">//证书的主题名称为etcd。</span></p><p id="u53e381f0" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">hosts   	</span><span class="ne-text" style="color: #DF2A3F">//定义可以使用此证书的主机IP地址。</span></p></details>

```shell
cat > etcd-csr.json <<"EOF"
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1",
    "192.168.0.111",
    "192.168.0.112",
    "192.168.0.113"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [{
    "C": "CN",
    "ST": "Beijing",
    "L": "Beijing",
    "O": "yesir",
    "OU": "CN"
  }]
}
EOF
```

使用现有的CA证书及其私钥 ，对`etcd-csr.json` 签名请求文件进行签署，生成一个新的https证书和对应的私钥

```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \
-config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
```

<details class="lake-collapse"><summary id="ubd82fcbd"><strong><span class="ne-text">命令说明：</span></strong></summary><p id="ua17ab13b" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">-ca=ca.pem 				</span><span class="ne-text" style="color: #DF2A3F">//参数指定了用于签名新证书的 CA 证书文件。</span></p><p id="u36c024e0" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">-ca-key=ca-key.pem 		</span><span class="ne-text" style="color: #DF2A3F">//参数指定了用于签名新证书的 CA 的私钥文件。</span></p><p id="u188128d6" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">-config=ca-config.json 		</span><span class="ne-text" style="color: #DF2A3F">//参数指定了证书的配置文件，它定义了 CA 的策略和默认设置。</span></p><p id="u71331d39" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">-profile=kubernetes		</span><span class="ne-text" style="color: #DF2A3F">//定义证书策略,策略名为（kubernetes）。</span></p><p id="u69a7e2c1" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">etcd-csr.json				</span><span class="ne-text" style="color: #DF2A3F">//证书签名请求文件。</span></p><p id="u0e3ac504" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">cfssljson -bare ca			</span><span class="ne-text" style="color: #DF2A3F">//将证书和私钥提取出来，分别保存为.pem文件。</span></p></details>

会生成如下证书与之配套的文件：

- `etcd.csr`	      //证书配套的签名请求文件, 包含证书中定义的一些信息。 
- `etcd-key.pem`   //证书配套的私钥文件, 验证证书的真实性。
- `etcd.pem`      //证书文件。

## 安装并配置etcd集群

下载etcd二进制包

<details class="lake-collapse"><summary id="u678dc039"><strong><span class="ne-text" style="font-size: 22px">下载地址：</span></strong></summary><p id="u6d49b696" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><a href="https://github.com/etcd-io/etcd/releases" data-href="https://github.com/etcd-io/etcd/releases" target="_blank" class="ne-link"><span class="ne-text" style="font-size: 16px">https://github.com/etcd-io/etcd/releases</span></a></p></details>

```shell
wget https://github.com/etcd-io/etcd/releases/download/v3.5.2/etcd-v3.5.2-linux-amd64.tar.gz
```

解压etcd软件包，并拷贝etcd相关工具到本机`/usr/local/bin/`

```shell
tar -xvf etcd-v3.5.2-linux-amd64.tar.gz && \
cp -p etcd-v3.5.2-linux-amd64/etcd* /usr/local/bin/
```

分发etcd工具到其他etcd节点

```shell
for etcd in master02 master03
do
	scp etcd-v3.5.2-linux-amd64/etcd* $etcd:/usr/local/bin/
done
```

创建目录用于存储etcd配置文件，该配置文件基于etcd内部的变量定义了etcd相关配置

提示：该配置文件不要随意添加注释，会导致服务无法启动。

```shell
mkdir /etc/etcd
cat >  /etc/etcd/etcd.conf <<"EOF"
#[Member]
ETCD_NAME="etcd1"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.0.111:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.0.111:2379,http://127.0.0.1:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.0.111:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.0.111:2379"
ETCD_INITIAL_CLUSTER="etcd1=https://192.168.0.111:2380,etcd2=https://192.168.0.112:2380,etcd3=https://192.168.0.113:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF
```

<details class="lake-collapse"><summary id="u6f1a4019"><strong><span class="ne-text">变量说明：</span></strong></summary><p id="u72e17b26" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ETCD_NAME							</span><span class="ne-text" style="color: #DF2A3F">//当前etcd实例的名称，集群中唯一。</span></p><p id="u8440ca95" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ETCD_DATA_DIR						</span><span class="ne-text" style="color: #DF2A3F">//etcd存储数据的目录。</span></p><p id="uf46cc203" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ETCD_LISTEN_PEER_URLS				</span><span class="ne-text" style="color: #DF2A3F">//定义集群内部通讯的地址及端口。</span></p><p id="u629333eb" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ETCD_LISTEN_CLIENT_URLS				</span><span class="ne-text" style="color: #DF2A3F">//定义监听客户端连接的地址及端口。</span></p><p id="ue046d6e8" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ETCD_INITIAL_ADVERTISE_PEER_URLS		</span><span class="ne-text" style="color: #DF2A3F">//向集群中的其他节点宣告自己的地址及端口。</span></p><p id="u0920a2ff" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ETCD_ADVERTISE_CLIENT_URLS			</span><span class="ne-text" style="color: #DF2A3F">//向客户端宣告自己的地址及端口。</span></p><p id="u1f4635f2" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ETCD_INITIAL_CLUSTER					</span><span class="ne-text" style="color: #DF2A3F">//初始化集群时所有etcd实例的列表。</span></p><p id="ufe9abecb" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ETCD_INITIAL_CLUSTER_TOKEN			</span><span class="ne-text" style="color: #DF2A3F">//集群Token名称，用于标识集群的唯一标记。</span></p><p id="u4f037862" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ETCD_INITIAL_CLUSTER_STATE			</span><span class="ne-text" style="color: #DF2A3F">//etcd集群的初始状态，new是新集群，existing表示加入已有集群。</span></p></details>

创建目录存放etcd证书

```shell
mkdir /etc/etcd/ssl
```

创建etcd数据目录

```shell
mkdir -p /var/lib/etcd/default.etcd
```

拷贝相关证书到etcd证书目录

```shell
cp ca*.pem /etc/etcd/ssl
cp etcd*.pem /etc/etcd/ssl
```

为etcd创建服务管理文件 (systemd管理etcd)

```shell
cat > /etc/systemd/system/etcd.service <<"EOF"
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
EnvironmentFile=-/etc/etcd/etcd.conf
WorkingDirectory=/var/lib/etcd/
ExecStart=/usr/local/bin/etcd \
  --cert-file=/etc/etcd/ssl/etcd.pem \
  --key-file=/etc/etcd/ssl/etcd-key.pem \
  --trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --peer-cert-file=/etc/etcd/ssl/etcd.pem \
  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \
  --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```

<details class="lake-collapse"><summary id="u20d664fa"><strong><span class="ne-text">[Service] 重要参数介绍：</span></strong></summary><p id="u5c8d20bd" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Type				</span><span class="ne-text" style="color: #DF2A3F">//服务启动类型，"notify" 表示服务启动完成后向Systemd发送一个通知。</span></p><p id="u337f04bc" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">EnvironmentFile		</span><span class="ne-text" style="color: #DF2A3F">//指定etcd环境变量文件，服务会在启动时加载文件中的环境变量。</span></p><p id="ubf515ec7" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">WorkingDirectory		</span><span class="ne-text" style="color: #DF2A3F">//定义了服务的工作目录，此处是 /var/lib/etcd/。</span></p><p id="u4697f21a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ExecStart 			</span><span class="ne-text" style="color: #DF2A3F">//定义了启动服务时要执行的命令，通过etcd的选项指定了相关证书。</span></p><p id="u45cba590" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Restart 				</span><span class="ne-text" style="color: #DF2A3F">//定义了服务失败时应如何操作，"on-failure" 在服务失败时重启服务。</span></p><p id="u16c9481f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">RestartSec			</span><span class="ne-text" style="color: #DF2A3F">//定义了服务失败后等待多久才重启服务，此处是 5 秒。</span></p><p id="uc769e79e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">LimitNOFILE			</span><span class="ne-text" style="color: #DF2A3F">//设置服务进程能打开的最大文件数，此处是 65536。</span></p></details>

在其他 etcd2 和 etcd3 节点创建好对应的目录

```shell
mkdir -p /etc/etcd
mkdir -p /etc/etcd/ssl
mkdir -p /var/lib/etcd/default.etcd
```

同步文件到etcd2 和 etcd3 节点

```shell
for etcd in master02 master03
do
scp /etc/etcd/etcd.conf $etcd:/etc/etcd
scp /etc/etcd/ssl/* $etcd:/etc/etcd/ssl
scp /etc/systemd/system/etcd.service $etcd:/etc/systemd/system
done
```

etcd2节点修改文件中的节点名称和IP地址

```shell
#[Member]
ETCD_NAME="etcd2"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.0.112:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.0.112:2379,http://127.0.0.1:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.0.112:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.0.112:2379"
ETCD_INITIAL_CLUSTER="etcd1=https://192.168.0.111:2380,etcd2=https://192.168.0.112:2380,etcd3=https://192.168.0.113:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
```

etcd3节点修改文件中的节点名称和IP地址

```shell
#[Member]
ETCD_NAME="etcd3"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.0.113:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.0.113:2379,http://127.0.0.1:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.0.113:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.0.113:2379"
ETCD_INITIAL_CLUSTER="etcd1=https://192.168.0.111:2380,etcd2=https://192.168.0.112:2380,etcd3=https://192.168.0.113:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
```

## 启动etcd并验证状态

所有etcd节点启动etcd

```shell
systemctl daemon-reload
systemctl start etcd
systemctl enable etcd
systemctl status etcd
```

验证集群状态

```shell
ETCDCTL_API=3 /usr/local/bin/etcdctl --write-out=table --cacert=/etc/etcd/ssl/ca.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints=https://192.168.0.111:2379,https://192.168.0.112:2379,https://192.168.0.113:2379 endpoint health
```

+------------------------------+------------+---------------+--------+
|          ENDPOINT                  | HEALTH     |    TOOK         | ERROR  |
+------------------------------+------------+---------------+--------+
| https://192.168.0.111:2379  |   true         | 13.048232ms  |            |
| https://192.168.0.113:2379  |   true         | 14.015441ms  |            |
| https://192.168.0.112:2379  |   true         |  14.34013ms   |            |
+------------------------------+------------+---------------+--------+



# 第四章：部署kube-apiserver组件

## 下载二进制软件包

<details class="lake-collapse"><summary id="u48458ba5"><strong><span class="ne-text" style="font-size: 22px">下载地址：</span></strong></summary><p id="ud1d22dd6" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md" data-href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md" target="_blank" class="ne-link"><span class="ne-text">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md</span></a></p></details>

![img](https://cdn.nlark.com/yuque/0/2024/png/44499768/1716645097538-d2033814-5ef2-48ce-be80-3b287321f606.png)

在master01节点下载

```shell
wget https://dl.k8s.io/v1.23.0/kubernetes-server-linux-amd64.tar.gz
```

解压软件包

```shell
tar -xvf kubernetes-server-linux-amd64.tar.gz && \
cd kubernetes/server/bin/
```

拷贝集群组件到`/usr/local/bin`

```shell
cp kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet kubectl /usr/local/bin
```

拷贝管理节点组件到master02、master03

```shell
for master in master02 master03
do
	scp kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet kubectl $master:/usr/local/bin
done
```

拷贝工作节点组件到node01、node2

```shell
for node in node01 node02
do
	scp kubelet kube-proxy $node:/usr/local/bin
done
```



**在集群所有 (不包括LB节点) 节点上创建一下目录**

```shell
#用于存储集群配置文件
mkdir -p /etc/kubernetes/        
#用于存储集群证书文件
mkdir -p /etc/kubernetes/ssl     
#用于存储集群日志文件
mkdir -p /var/log/kubernetes
```

## 创建Api Server证书

生成Api Server证书签名请求文件

<details class="lake-collapse"><summary id="ue6e4cf18"><strong><span class="ne-text">参数说明：</span></strong></summary><p id="ue5978db9" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">CN		</span><span class="ne-text" style="color: #DF2A3F">//证书的主题名称</span></p><p id="ud2f5efe6" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">hosts   	</span><span class="ne-text" style="color: #DF2A3F">//定义可以使用此证书的主机IP（含VIP）地址，方便后期扩容可以多预留一些IP，10.96.0.1是k8s服务发现机制使用的IP</span></p><p id="ub312d96a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="color: #DF2A3F">"kubernetes"、"kubernetes.default"、"kubernetes.default.svc"、"kubernetes.default.svc.cluster" 、"kubernetes.default.svc.cluster.local" 是k8s集群中用于访问 API  的域名和全域名。</span></p></details>

```shell
cd /root/work
cat > kube-apiserver-csr.json << "EOF"
{
"CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "192.168.0.111",
    "192.168.0.112",
    "192.168.0.113",
    "192.168.0.114",
    "192.168.0.115",
    "192.168.0.116",
    "192.168.0.100",
    "10.96.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "yesir",
      "OU": "CN"
    }
  ]
}
EOF
```

使用现有的CA证书及其私钥 ，对`kube-apiserver-csr.json` 文件进行签署，生成一个新的https证书和对应的私钥

```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver
```

会生成如下证书与之配套的文件：

- `kube-apiserver.csr`		   //证书配套的签名请求文件, 包含证书中定义的一些信息 
- `kube-apiserver-key.pem`    //证书配套的私钥文件, 验证证书的真实性 
- `kube-apiserver.pem`       //证书文件 



创建一个token（令牌）文件，k8s引入了TLS bootstraping机制来自动颁发kubelet证书，当Node节点很多时，来简化证书颁发流程（在kubelet章节在详细讲解）

```shell
cat > token.csv << EOF
$(head -c 16 /dev/urandom | od -An -t x | tr -d ' '),kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF
```

<details class="lake-collapse"><summary id="uf7ee1ca6"><strong><span class="ne-text">格式介绍：</span></strong></summary><p id="u874623d2" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')	</span><span class="ne-text" style="color: #DF2A3F">//生成Token</span></p><p id="u8060970f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">kubelet-bootstrap								</span><span class="ne-text" style="color: #DF2A3F">//用户名</span></p><p id="u35c2e5c5" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">10001										</span><span class="ne-text" style="color: #DF2A3F">//UID</span></p><p id="ua90cc06c" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">"system:kubelet-bootstrap"						</span><span class="ne-text" style="color: #DF2A3F">//用户组</span></p></details>

## 创建Api Server配置文件

该配置文件基于Api Server内部的变量定义了相关配置

```shell
cat > /etc/kubernetes/kube-apiserver.conf << "EOF"
KUBE_APISERVER_OPTS="--enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
  --anonymous-auth=false \
  --bind-address=192.168.0.111 \
  --secure-port=6443 \
  --advertise-address=192.168.0.111 \
  --insecure-port=0 \
  --authorization-mode=Node,RBAC \
  --runtime-config=api/all=true \
  --enable-bootstrap-token-auth \
  --service-cluster-ip-range=10.96.0.0/16 \
  --token-auth-file=/etc/kubernetes/token.csv \
  --service-node-port-range=30000-32767 \
  --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem  \
  --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \
  --client-ca-file=/etc/kubernetes/ssl/ca.pem \
  --kubelet-client-certificate=/etc/kubernetes/ssl/kube-apiserver.pem \
  --kubelet-client-key=/etc/kubernetes/ssl/kube-apiserver-key.pem \
  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --service-account-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  \
  --service-account-issuer=api \
  --etcd-cafile=/etc/etcd/ssl/ca.pem \
  --etcd-certfile=/etc/etcd/ssl/etcd.pem \
  --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \
  --etcd-servers=https://192.168.0.111:2379,https://192.168.0.112:2379,https://192.168.0.113:2379 \
  --enable-swagger-ui=true \
  --allow-privileged=true \
  --apiserver-count=3 \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/log/kube-apiserver-audit.log \
  --event-ttl=1h \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=4"
EOF
```

<details class="lake-collapse"><summary id="ueb9132df"><strong><span class="ne-text">插件说明：</span></strong></summary><p id="u5dc560f8" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--enable-admission-plugins=		</span><span class="ne-text" style="color: #DF2A3F">//启用一组准入控制插件，以下是各个插件的作用：</span></p><ul class="ne-ul" style="margin: 0; padding-left: 23px"><li id="uf12ee9ab" data-lake-index-type="0"><span class="ne-text">NamespaceLifecycle			</span><span class="ne-text" style="color: #DF2A3F">//该插件会拒绝那些正在终止的命名空间中的请求</span></li><li id="u83fcf217" data-lake-index-type="0"><span class="ne-text">NodeRestriction				</span><span class="ne-text" style="color: #DF2A3F">//该插件限制kubelet只能管理自己的节点的Pod</span></li><li id="u00661beb" data-lake-index-type="0"><span class="ne-text">LimitRanger					</span><span class="ne-text" style="color: #DF2A3F">//该插件实现资源使用的默认限制和配额</span></li><li id="u006ffb3f" data-lake-index-type="0"><span class="ne-text">ServiceAccount				</span><span class="ne-text" style="color: #DF2A3F">//该插件则确保在创建的每个Pod中都有一个ServiceAccount</span></li><li id="uaf1ebee1" data-lake-index-type="0"><span class="ne-text">DefaultStorageClass			</span><span class="ne-text" style="color: #DF2A3F">//该插件实现动态存储(StorageClass)的PVC绑定</span></li><li id="u2ce6d067" data-lake-index-type="0"><span class="ne-text">ResourceQuota				</span><span class="ne-text" style="color: #DF2A3F">//该插件实现对命名空间的资源使用量设定配额</span></li></ul></details>

<details class="lake-collapse"><summary id="ue28a6159"><strong><span class="ne-text">选项说明：</span></strong></summary><p id="u2df48197" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--anonymous-auth=false			</span><span class="ne-text" style="color: #DF2A3F">//禁止匿名用户访问API</span></p><p id="ue7276081" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--bind-address					</span><span class="ne-text" style="color: #DF2A3F">//API服务器监听的IP地址</span></p><p id="uf50f717e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--secure-port						</span><span class="ne-text" style="color: #DF2A3F">//API服务器监听HTTPS连接的端口</span></p><p id="u19673d6e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--advertise-address				</span><span class="ne-text" style="color: #DF2A3F">//这是API服务器公告给集群中其它组件的IP地址</span></p><p id="u719c8446" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--insecure-port					</span><span class="ne-text" style="color: #DF2A3F">//此选项设置为0表示API服务器监听不安全的HTTP连接</span></p><p id="ue147e469" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--authorization-mode				</span><span class="ne-text" style="color: #DF2A3F">//这是API服务器使用的授权模式</span></p><p id="u44fbd88a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--runtime-config					</span><span class="ne-text" style="color: #DF2A3F">//这个选项用于启用或禁用API版本或特定的API资源</span></p><p id="ue37ea8ea" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--enable-bootstrap-token-auth		</span><span class="ne-text" style="color: #DF2A3F">//启用用于节点引导的令牌认证</span></p><p id="u4a7f600c" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--service-cluster-ip-range			</span><span class="ne-text" style="color: #DF2A3F">//为Service分配的IP地址范围</span></p><p id="u9e09877c" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--token-auth-file					</span><span class="ne-text" style="color: #DF2A3F">//此文件用于通过令牌进行身份验证</span></p><p id="u86654384" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--service-node-port-range			</span><span class="ne-text" style="color: #DF2A3F">//为NodePort Service分配的端口范围</span></p><p id="ufcfb31e2" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--service-account-issuer				</span><span class="ne-text" style="color: #DF2A3F">//ServiceAccount令牌的颁发者</span></p><p id="u15c2582f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--enable-swagger-ui				</span><span class="ne-text" style="color: #DF2A3F">//启用可视化功能可通过浏览器向你的API发送请求</span></p><p id="u41104be1" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--allow-privileged					</span><span class="ne-text" style="color: #DF2A3F">//是否允许创建具有特权的容器</span></p><p id="u94f4ab07" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--apiserver-count					</span><span class="ne-text" style="color: #DF2A3F">//用于端点选举的API服务器的数量</span></p><p id="uef2bd3b8" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--audit-log-maxage、--audit-log-maxbackup、--audit-log-maxsize、--audit-log-path								 		</span><span class="ne-text" style="color: #DF2A3F">//这些选项配置了审计日志功能</span></p><p id="u802780b7" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--event-ttl=1h</span><span class="ne-text" style="color: #DF2A3F">                                        //日志保留时长为 1 小时， 之后将被自动删除，避免日志占用大量磁盘</span></p><p id="uf5a8cf06" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--alsologtostderr=true				</span><span class="ne-text" style="color: #DF2A3F">//将日志输出到标准错误（stderr）避免日志丢失</span></p><p id="u6c82b53f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--logtostderr=false					</span><span class="ne-text" style="color: #DF2A3F">//生成日志文件（不加这个参数不会生成日志）</span></p><p id="u1162788a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--log-dir=/var/log/kubernetes		</span><span class="ne-text" style="color: #DF2A3F">//指定kube-apiserver将日志写入到的目录</span></p><p id="u8e9f2062" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--v=4							</span><span class="ne-text" style="color: #DF2A3F">//日志等级,数值越大,输出的日志越详细</span></p></details>

## 创建apiserver服务管理文件

通过systemd管理Api Server的启动或关闭

```shell
cat > /etc/systemd/system/kube-apiserver.service << "EOF"
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=etcd.service
Wants=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/kube-apiserver.conf
ExecStart=/usr/local/bin/kube-apiserver $KUBE_APISERVER_OPTS
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```

<details class="lake-collapse"><summary id="udc82a78a"><strong><span class="ne-text">参数说明：</span></strong></summary><p id="ube0e3bb8" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text">[Unit] </span></strong></p><p id="u3d318071" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Description		</span><span class="ne-text" style="color: #DF2A3F">//服务描述</span></p><p id="u7f85c4aa" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Documentation	</span><span class="ne-text" style="color: #DF2A3F">//服务文档的链接</span></p><p id="uf3424f81" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">After			</span><span class="ne-text" style="color: #DF2A3F">//这个服务会在etcd.service启动后启动</span></p><p id="u3765392b" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Wants			</span><span class="ne-text" style="color: #DF2A3F">//如果etcd.service服务失败，这个服务仍会尝试启动</span></p><p id="u7a9429d6" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text"></span></p><p id="u98dc0442" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text">[Service] </span></strong></p><p id="u880bf2a7" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">EnvironmentFile	</span><span class="ne-text" style="color: #DF2A3F">//指向服务的配置文件</span></p><p id="u6cdba6c5" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ExecStart			</span><span class="ne-text" style="color: #DF2A3F">//定义了启动服务时运行的命令，基于$KUBE_APISERVER_OPTS变量中选项运行</span></p><p id="u5aee2fbe" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Restart			</span><span class="ne-text" style="color: #DF2A3F">//如果服务进程失败（退出状态非零）systemd将重新启动它</span></p><p id="u713e66db" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">RestartSec		</span><span class="ne-text" style="color: #DF2A3F">//定义了在服务重启之间的延迟,如果服务需要重启，systemd将等待5秒</span></p><p id="ua1917855" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Type			</span><span class="ne-text" style="color: #DF2A3F">//定义了服务的类型，notify表示服务在准备好接受请求时给systemd发送通知</span></p><p id="u8c3b51a2" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">LimitNOFILE		</span><span class="ne-text" style="color: #DF2A3F">//定义了服务进程可以打开的最大文件数量</span></p></details>

同步文件到本机对应目录

```shell
cp ca*.pem /etc/kubernetes/ssl/
cp kube-apiserver*.pem /etc/kubernetes/ssl/
cp token.csv /etc/kubernetes/
```

同步文件到其他Master节点

```shell
for master in master02 master03
do
	scp /etc/kubernetes/kube-apiserver.conf $master:/etc/kubernetes
	scp /etc/kubernetes/ssl/ca*.pem $master:/etc/kubernetes/ssl
	scp /etc/kubernetes/ssl/kube-apiserver*.pem $master:/etc/kubernetes/ssl
	scp /etc/kubernetes/token.csv $master:/etc/kubernetes
	scp /etc/systemd/system/kube-apiserver.service $master:/etc/systemd/system
done
```

Master02节点需要修改文件中的`--bind-address与--advertise-address` IP为本机

```shell
vim /etc/kubernetes/kube-apiserver.conf
#...
  --bind-address=192.168.0.112 \
  --advertise-address=192.168.0.112 \
```

Master03节点需要修改文件中的`--bind-address与--advertise-address` IP为本机

```shell
vim /etc/kubernetes/kube-apiserver.conf
#...
  --bind-address=192.168.0.113 \
  --advertise-address=192.168.0.113 \
```

## 启动Api Server服务

```shell
systemctl daemon-reload
systemctl start kube-apiserver
systemctl enable kube-apiserver
systemctl status kube-apiserver
```

# 第五章：部署kubectl组件

## 创建kubectl证书

生成kubectl证书签名请求文件

```shell
cat > admin-csr.json << "EOF"
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:masters",             
      "OU": "system"
    }
  ]
}
EOF
```

**说明：**这个证书是用来生成集群管理员的配置文`kube.config`，且证书中的“O”（公司）参数必须是`system:masters`，否则后面`kubectl create clusterrolebinding`报错，则无法生成`kube.config`文件。



使用现有的CA证书及其私钥 ，对`admin-csr.json` 证书签名请求文件进行签署，生成一个新的https证书和对应的私钥

```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
```

会生成如下证书与之配套的文件：

-  `admin.csr`			//证书配套的签名请求文件, 包含证书中定义的一些信息 
-  `admin-key.pem`     //证书配套的私钥文件, 验证证书的真实性 
-  `admin.pem`         //证书文件 



拷贝证书文件到本机`/etc/kubernetes/ssl`

```shell
cp admin*.pem /etc/kubernetes/ssl
```

## 配置kubectl上下文![img](https://cdn.nlark.com/yuque/0/2024/png/48193331/1729517617813-ca637a9b-ce4d-4d3d-bbd8-7e3ca4dcc1e5.png)

上下文是用于记录集群信息，包括，集群名称、集群地址及端口、集群证书、集群用户等信息，kubectl 基于这些信息来访问和管理集群。

在`kube.config`文件中指定集群名称、集群地址及端口（端口为 VIP 通信端口）

```shell
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.0.100:6443 --kubeconfig=kube.config
```

<details class="lake-collapse"><summary id="u55d1df12"><strong><span class="ne-text">命令说明：</span></strong></summary><p id="ud5086580" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">kubectl config set-cluster 			</span><span class="ne-text" style="color: #DF2A3F">//设置集群，kubernetes是集群名称</span></p><p id="u8435eacb" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--certificate-authority=ca.pem		</span><span class="ne-text" style="color: #DF2A3F">//验证API server的证书文件</span></p><p id="u5e59ff88" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--embed-certs=true				</span><span class="ne-text" style="color: #DF2A3F">//将证书信息嵌入到kube.config文件中，这样即使证书文件移动或删除，kube.config文件仍然有效</span></p><p id="u2c43737a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--server=</span><a href="https://192.168.0.100:6443" data-href="https://192.168.0.100:6443" target="_blank" class="ne-link"><span class="ne-text">https://192.168.0.100:6443</span></a><span class="ne-text">	</span><span class="ne-text" style="color: #DF2A3F">//这个参数指定了API Server的地址（VIP地址）</span></p><p id="ucb274a0b" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="color: #DF2A3F">kube.config						//文件名</span></p></details>

在`kube.config`文件中增加admin用户（集群管理员）

```shell
kubectl config set-credentials admin --client-certificate=admin.pem --client-key=admin-key.pem --embed-certs=true --kubeconfig=kube.config
```

<details class="lake-collapse"><summary id="u3874749b"><strong><span class="ne-text">命令说明：</span></strong></summary><p id="u696d80d6" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">kubectl config set-credentials		</span><span class="ne-text" style="color: #DF2A3F">//设置集群用户，用户名admin</span></p><p id="ueae2fc8a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--client-certificate=admin.pem		</span><span class="ne-text" style="color: #DF2A3F">//验证admin的证书文件</span></p><p id="u74f17d78" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--client-key=admin-key.pem			</span><span class="ne-text" style="color: #DF2A3F">//验证admin的私钥文件</span></p><p id="uf1d76ba6" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--embed-certs=true				</span><span class="ne-text" style="color: #DF2A3F">//将证书数据嵌入到kube.config文件中，这样即使证书文件移动或删除，kube.config文件仍然有效</span></p><p id="u817c0719" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--kubeconfig=kube.config			</span><span class="ne-text" style="color: #DF2A3F">//文件名</span></p></details>

创建kubectl上下文信息

```shell
kubectl config set-context admin --cluster=kubernetes --user=admin --kubeconfig=kube.config
```

<details class="lake-collapse"><summary id="u5abd5735"><strong><span class="ne-text">命令说明：</span></strong></summary><p id="udb5ee36f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">kubectl config set-context 			</span><span class="ne-text" style="color: #DF2A3F">//设置集群上下文</span></p><p id="ucdf0d54c" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--cluster=kubernetes				</span><span class="ne-text" style="color: #DF2A3F">//集群名称</span></p><p id="u76fb3e36" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--user=admin						</span><span class="ne-text" style="color: #DF2A3F">//用户名</span></p><p id="u0920ebb8" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--kubeconfig=kube.config			</span><span class="ne-text" style="color: #DF2A3F">//指定了将被修改的kubeconfig文件名</span></p></details>

使用admin用户切换到集群验证

```shell
kubectl config use-context admin --kubeconfig=kube.config
```

拷贝`kube.config`文件到`/root/.kube`目录 (该文件在哪个用户家目录，用户就可以基于文件的admin权限管理集群)

```shell
mkdir /root/.kube && \
cp kube.config /root/.kube/config
```

创建ApiServer到kubelet的权限，否则无法对Pod容器执行查看、进入等权限

```shell
kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes --kubeconfig=/root/.kube/config
```

## 查看集群状态信息

查看正在运行的集群的基本信息

```shell
kubectl cluster-info
===========================================================
Kubernetes control plane is running at https://192.168.0.100:6443
```

检查集群的各个管理节点组件的健康状况

```shell
kubectl get cs
===========================================================
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                        ERROR
scheduler            Unhealthy   Get "https://127.0.0.1:10259/healthz": dial tcp 127.0.0.1:10259: connect: connection refused   
controller-manager   Unhealthy   Get "https://127.0.0.1:10257/healthz": dial tcp 127.0.0.1:10257: connect: connection refused   
etcd-0               Healthy     ok 
```

同步文件到其他Master节点，提前在其他节点创建好`.kube`目录

```shell
mkdir /root/.kube
```

同步`config`文件到其他管理节点

```shell
for master in master02 master03
do
	scp /root/.kube/config $master:/root/.kube/config
done
```

# 第六章：部署Controller Manager组件

## 生成controller-manager相关证书

生成controller-manager证书签名请求文件

```shell
cat > kube-controller-manager-csr.json << "EOF"
{
    "CN": "system:kube-controller-manager",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "hosts": [
      "127.0.0.1",
      "192.168.0.111",
      "192.168.0.112",
      "192.168.0.113"
    ],
    "names": [
      {
        "C": "CN",
        "ST": "Beijing",
        "L": "Beijing",
        "O": "system:kube-controller-manager",
        "OU": "system"
      }
    ]
}
EOF
```

<details class="lake-collapse"><summary id="u4c52ef7d"><strong><span class="ne-text">参数说明：</span></strong></summary><p id="uaab40f89" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">hosts			</span><span class="ne-text" style="color: #DF2A3F">//包含所有controller-manager节点IP</span></p><p id="ue1fe23f7" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">O（公司） 	 	</span><span class="ne-text" style="color: #DF2A3F">//system:kube-controller-manager是集群内置的ClusterRoleBindings，赋予 kube-controller-manager所需的权限</span></p></details>

使用现有的CA证书及其私钥 ，对`kube-controller-manager-csr.json` 文件进行签署，生成一个新的https证书和对应的私钥

```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
```

会生成如下证书与之配套的文件：

-  `kube-controller-manager.csr`		   //证书配套的签名请求文件, 包含证书中定义的一些信息 
-  `kube-controller-manager-key.pem`   //证书配套的私钥文件, 验证证书的真实性 
-  `kube-controller-manager.pem`       //证书文件 

## 配置controller-manager上下文

在`kube-controller-manager.kubeconfig`文件中指定集群名称、集群地址及端口

```shell
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.0.100:6443 --kubeconfig=kube-controller-manager.kubeconfig
```

在`kube-controller-manager.kubeconfig`文件中增加 `system:kube-controller-manager` 用户

```shell
kubectl config set-credentials system:kube-controller-manager --client-certificate=kube-controller-manager.pem --client-key=kube-controller-manager-key.pem --embed-certs=true --kubeconfig=kube-controller-manager.kubeconfig
```

创建controller-manager上下文信息

```shell
kubectl config set-context system:kube-controller-manager --cluster=kubernetes --user=system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
```

使用system:kube-controller-manager切换到集群验证

```shell
kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
```

## 创建controller-manager服务配置文件

该配置文件基于controller-manager内部的变量定义了相关配置

```shell
cat > kube-controller-manager.conf << "EOF"
KUBE_CONTROLLER_MANAGER_OPTS="--port=0 \
  --secure-port=10257 \
  --bind-address=127.0.0.1 \
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \
  --service-cluster-ip-range=10.96.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \
  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --allocate-node-cidrs=true \
  --cluster-cidr=10.244.0.0/16 \
  --experimental-cluster-signing-duration=87600h \
  --root-ca-file=/etc/kubernetes/ssl/ca.pem \
  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --leader-elect=true \
  --feature-gates=RotateKubeletServerCertificate=true \
  --controllers=*,bootstrapsigner,tokencleaner \
  --horizontal-pod-autoscaler-sync-period=10s \
  --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \
  --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \
  --use-service-account-credentials=true \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2"
EOF
```

<details class="lake-collapse"><summary id="u86c7a8c7"><strong><span class="ne-text">选项说明：</span></strong></summary><p id="u8f1e2e7c" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--port=0						</span><span class="ne-text" style="color: #DF2A3F">//关闭不安全HTTP端口访问</span></p><p id="u3607353e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--secure-port					</span><span class="ne-text" style="color: #DF2A3F">//kube-controller-manager的HTTPS端口</span></p><p id="u8b8dc172" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--bind-address				</span><span class="ne-text" style="color: #DF2A3F">//kube-controller-manager绑定的本机IP地址</span></p><p id="u6653416f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--kubeconfig					</span><span class="ne-text" style="color: #DF2A3F">//kube-controller-manager上下文，用于跟API server通信</span></p><p id="u89af515e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--service-cluster-ip-range		</span><span class="ne-text" style="color: #DF2A3F">//指定服务IP地址范围</span></p><p id="u210ae501" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--cluster-name				</span><span class="ne-text" style="color: #DF2A3F">//指定当前集群的名称</span></p><p id="uf0f23e06" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--allocate-node-cidrs			</span><span class="ne-text" style="color: #DF2A3F">//启动集群CIDR（即子网）用于Pod网络</span></p><p id="ue7ccd363" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--cluster-cidr					</span><span class="ne-text" style="color: #DF2A3F">//集群的CIDR（即子网）范围</span></p><p id="u9248f52a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--experimental-cluster-signing-duration=87600h		</span><span class="ne-text" style="color: #DF2A3F">//证书签名的持续时间</span></p><p id="u75e8c879" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--leader-elect					</span><span class="ne-text" style="color: #DF2A3F">//启用leader（领导者）选举，实现集群高可用性</span></p><p id="u0f34ce92" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--feature-gates=RotateKubeletServerCertificate		</span><span class="ne-text" style="color: #DF2A3F">//启用自动管理和更新Kubelet的TLS证书</span></p><p id="u792cf86a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--controllers					</span><span class="ne-text" style="color: #DF2A3F">//表示启动所有默认的控制器(*)，并额外启动bootstrapsigner 和tokencleaner控制器,主要用于身份验证</span></p><p id="u859a974e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--horizontal-pod-autoscaler-sync-period		</span><span class="ne-text" style="color: #DF2A3F">//HPA控制器规则检测时间</span></p><p id="uc3c37d85" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--use-service-account-credentials				</span><span class="ne-text" style="color: #DF2A3F">//每个控制使用单独的服务帐户凭证进行身份验证</span></p><p id="u4a21d991" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--alsologtostderr							</span><span class="ne-text" style="color: #DF2A3F">//日志写入到标准错误，避免日志丢失</span></p></details>

## 创建controller-manager服务管理文件

```shell
cat > kube-controller-manager.service << "EOF"
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/kube-controller-manager.conf
ExecStart=/usr/local/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTS
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

同步文件到本机对应目录

```shell
cp kube-controller-manager*.pem /etc/kubernetes/ssl/
cp kube-controller-manager.kubeconfig /etc/kubernetes/
cp kube-controller-manager.conf /etc/kubernetes/
cp kube-controller-manager.service /usr/lib/systemd/system/
```

同步文件到其他Master节点

```shell
for master in master02 master03
do
	scp  kube-controller-manager*.pem $master:/etc/kubernetes/ssl/
	scp  kube-controller-manager.kubeconfig $master:/etc/kubernetes/
	scp  kube-controller-manager.conf $master:/etc/kubernetes/
	scp  kube-controller-manager.service $master:/usr/lib/systemd/system/
done
```

## 启动controller-manager服务

```shell
systemctl daemon-reload
systemctl start kube-controller-manager
systemctl enable kube-controller-manager
systemctl status kube-controller-manager
```

# 第七章：部署Scheduler组件

## 生成scheduler相关证书

生成scheduler证书签名请求文件

```shell
cat > kube-scheduler-csr.json << "EOF"
{
    "CN": "system:kube-scheduler",
    "hosts": [
      "127.0.0.1",
      "192.168.0.111",
      "192.168.0.112",
      "192.168.0.113"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "Beijing",
        "L": "Beijing",
        "O": "system:kube-scheduler",
        "OU": "system"
      }
    ]
}
EOF
```

<details class="lake-collapse"><summary id="u38d86d0e"><strong><span class="ne-text">关键参数说明：</span></strong></summary><p id="ub00a4d14" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">hosts			</span><span class="ne-text" style="color: #DF2A3F">//包含所有scheduler节点IP。</span></p><p id="u4356de6c" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">O（公司） 	 	</span><span class="ne-text" style="color: #DF2A3F">//system:kube-scheduler名字不要修改，后边添加该用户，用于赋予kube-scheduler工作所需的权限。</span></p></details>

使用现有的CA证书及其私钥 ，对`kube-scheduler-csr.json`文件进行签署，生成一个新的https证书和对应的私钥

```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
```

会生成如下证书与之配套的文件：

-  `kube-scheduler.csr`			//证书配套的签名请求文件, 包含证书中定义的一些信息 
-  `kube-scheduler-key.pem`   //证书配套的私钥文件, 验证证书的真实性 
-  `kube-scheduler.pem`       	//证书文件 

## 配置scheduler上下文

在`kube-scheduler.kubeconfig`文件中指定集群名称、集群地址及端口

```shell
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.0.100:6443 --kubeconfig=kube-scheduler.kubeconfig
```

在`kube-scheduler.kubeconfig`文件中增加 `system:kube-scheduler` 用户

```shell
kubectl config set-credentials system:kube-scheduler --client-certificate=kube-scheduler.pem --client-key=kube-scheduler-key.pem --embed-certs=true --kubeconfig=kube-scheduler.kubeconfig
```

创建scheduler上下文信息

```shell
kubectl config set-context system:kube-scheduler --cluster=kubernetes --user=system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
```

使用system:kube-scheduler切换到集群验证

```shell
kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
```

## 创建scheduler服务配置文件

该配置文件基于scheduler内部的变量定义了相关配置

```shell
cat > kube-scheduler.conf << "EOF"
KUBE_SCHEDULER_OPTS="--address=127.0.0.1 \
--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
--leader-elect=true \
--alsologtostderr=true \
--logtostderr=false \
--log-dir=/var/log/kubernetes \
--v=2"
EOF
```

## 创建scheduler服务管理文件

```shell
cat > kube-scheduler.service << "EOF"
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/kube-scheduler.conf
ExecStart=/usr/local/bin/kube-scheduler $KUBE_SCHEDULER_OPTS
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

同步文件到本机对应目录

```shell
cp kube-scheduler*.pem /etc/kubernetes/ssl/
cp kube-scheduler.kubeconfig /etc/kubernetes/
cp kube-scheduler.conf /etc/kubernetes/
cp kube-scheduler.service /usr/lib/systemd/system/
```

同步文件到其他Master节点

```shell
for master in master02 master03
do
	scp  kube-scheduler*.pem $master:/etc/kubernetes/ssl/
	scp  kube-scheduler.kubeconfig $master:/etc/kubernetes/
	scp  kube-scheduler.conf $master:/etc/kubernetes/
	scp  kube-scheduler.service $master:/usr/lib/systemd/system/
done
```

## 启动scheduler服务

```shell
systemctl daemon-reload
systemctl start kube-scheduler
systemctl enable kube-scheduler
systemctl status kube-scheduler
```

# 第八章：部署kubelet组件

## 定义kubelet bootstrap

当集群工作节点数量较多时，为每个节点手动颁发kubelet证书非常不方便，k8s引入了bootstraping引导机制来简化证书颁发流程。

kubelet 会使用一个预先提供的 "bootstrap" 证书来向 Api Server 发送一个证书签名请求 (CSR) ，从而自动生成 kubelet 的证书。

```shell
BOOTSTRAP_TOKEN=$(awk -F "," '{print $1}' /etc/kubernetes/token.csv)
```

## 配置kubelet上下文

在`kubelet-bootstrap.kubeconfig`文件中指定集群名称、集群地址及端口

```shell
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.0.100:6443 --kubeconfig=kubelet-bootstrap.kubeconfig
```

在`kubelet-bootstrap.kubeconfig`文件中增加 ` kubelet-bootstrap` 用户

```shell
kubectl config set-credentials kubelet-bootstrap --token=${BOOTSTRAP_TOKEN} --kubeconfig=kubelet-bootstrap.kubeconfig
```

创建kubelet上下文信息

```shell
kubectl config set-context default --cluster=kubernetes --user=kubelet-bootstrap --kubeconfig=kubelet-bootstrap.kubeconfig
```

通过文件切换到集群验证

```shell
kubectl config use-context default --kubeconfig=kubelet-bootstrap.kubeconfig
```

创建集群角色绑定，将`cluster-admin`管理员角色绑定到kubelet-bootstrap，system:node-bootstrapper 是一个特殊的集群角色，它用于在节点加入集群时授予临时的权限，以便节点可以自动注册到集群。

```shell
kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=kubelet-bootstrap
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap --kubeconfig=kubelet-bootstrap.kubeconfig
```

## 创建kubelet服务配置文件

Kubelet 的配置文件，定义了 Kubelet 运行所需的一些配置参数

```shell
cat > kubelet.json << "EOF"
{
  "kind": "KubeletConfiguration",
  "apiVersion": "kubelet.config.k8s.io/v1beta1",
  "authentication": {
    "x509": {
      "clientCAFile": "/etc/kubernetes/ssl/ca.pem"
    },
    "webhook": {
      "enabled": true,
      "cacheTTL": "2m0s"
    },
    "anonymous": {
      "enabled": false
    }
  },
  "authorization": {
    "mode": "Webhook",
    "webhook": {
      "cacheAuthorizedTTL": "5m0s",
      "cacheUnauthorizedTTL": "30s"
    }
  },
  "address": "192.168.0.111",
  "port": 10250,
  "readOnlyPort": 10255,
  "cgroupDriver": "systemd",                    
  "hairpinMode": "promiscuous-bridge",
  "serializeImagePulls": false,
  "clusterDomain": "cluster.local.",
  "clusterDNS": ["10.96.0.2"]
}
EOF
```

<details class="lake-collapse"><summary id="u3c5978ff"><strong><span class="ne-text">参数说明：</span></strong></summary><p id="u4ff563fc" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">kind					</span><span class="ne-text" style="color: #DF2A3F">//说明这个文档定义的是Kubelet配置</span></p><p id="udaec31a5" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">apiVersion			/</span><span class="ne-text" style="color: #DF2A3F">/配置文件遵循的API版本</span></p><p id="u9206c3ad" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">authentication			</span><span class="ne-text" style="color: #DF2A3F">//配置Kubelet的身份验证设置，包括 x509、webhook和匿名三种方式</span></p><p id="uce84a756" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">x509				</span><span class="ne-text" style="color: #DF2A3F">//用于验证来自API服务器的请求的CA证书文件</span></p><p id="u166a44c5" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">webhook				</span><span class="ne-text" style="color: #DF2A3F">//启用基于webhook的身份验证和缓存TTL</span></p><p id="ue5ab7c17" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">anonymous			</span><span class="ne-text" style="color: #DF2A3F">//关闭匿名用户的访问</span></p><p id="u7c637592" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">authorization			</span><span class="ne-text" style="color: #DF2A3F">//配置Kubelet的授权模式为Webhook,设置授权和未授权的缓存TTL</span></p><p id="u71b99bfd" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">address 				</span><span class="ne-text" style="color: #DF2A3F">//Kubelet服务监听的IP地址</span></p><p id="u7f94199c" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">port					</span><span class="ne-text" style="color: #DF2A3F">//Kubelet服务监听的端口号</span></p><p id="u2c6908e6" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">readOnlyPort			</span><span class="ne-text" style="color: #DF2A3F">//Kubelet只读端口号</span></p><p id="uc8ef12f8" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">cgroupDriver			</span><span class="ne-text" style="color: #DF2A3F">//Kubelet使用的cgroup驱动，用于限制容器的资源使用量</span></p><p id="uc7d00a55" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">hairpinMode			</span><span class="ne-text" style="color: #DF2A3F">//bridge实现Pod中的不同容器间通信</span></p><p id="u8ab9a59f" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">serializeImagePulls		</span><span class="ne-text" style="color: #DF2A3F">//此设置为false表示允许Kubelet并行拉取镜像</span></p><p id="u0cac64ec" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">clusterDomain			</span><span class="ne-text" style="color: #DF2A3F">//设置Kubernetes集群的DNS域名</span></p><p id="u9a9f7b8e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">clusterDNS			</span><span class="ne-text" style="color: #DF2A3F">//设置集群DNS服务器的地址</span></p></details>

## 创建kubelet服务管理文件

```shell
cat > kubelet.service << "EOF"
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/local/bin/kubelet \
  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \
  --cert-dir=/etc/kubernetes/ssl \
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
  --config=/etc/kubernetes/kubelet.json \
  --network-plugin=cni \
  --rotate-certificates \
  --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.6 \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

<details class="lake-collapse"><summary id="u183e8e39"><strong><span class="ne-text">参数说明：</span></strong></summary><p id="u6822b497" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text">[Unit]</span></strong></p><p id="u0ea2bd5d" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Description					</span><span class="ne-text" style="color: #DF2A3F">//服务的简要描述</span></p><p id="u47305942" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Documentation				</span><span class="ne-text" style="color: #DF2A3F">//服务的文档链接</span></p><p id="u48473e1e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">After						</span><span class="ne-text" style="color: #DF2A3F">//定义了该服务启动应该在哪些服务之后</span></p><p id="u6df06361" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Requires						</span><span class="ne-text" style="color: #DF2A3F">//定义了该服务启动需要哪些服务</span></p><p id="ub25285cc" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text"></span></p><p id="ud22db0bb" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text">[Service]</span></strong><span class="ne-text">						</span><span class="ne-text" style="color: #DF2A3F">//定义了服务的启动细节</span></p><p id="u67e44116" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">WorkingDirectory				</span><span class="ne-text" style="color: #DF2A3F">//定义了Kubelet的工作目录</span></p><p id="uc3f61d65" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">ExecStart						</span><span class="ne-text" style="color: #DF2A3F">//定义了启动服务的命令和参数</span></p><p id="ub09f9740" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--network-plugin				</span><span class="ne-text" style="color: #DF2A3F">//指定kubelet使用的网络插件</span></p><p id="u4f1a14d4" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--rotate-certificates			</span><span class="ne-text" style="color: #DF2A3F">//启用kubelet证书轮换，证书有效期限结束时自动请求新的证书</span></p><p id="u355c8334" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--pod-infra-container-image		</span><span class="ne-text" style="color: #DF2A3F">//指定pause容器的镜像</span></p><p id="ua57210e9" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--alsologtostderr				</span><span class="ne-text" style="color: #DF2A3F">//日志输出到标准错误，避免日志丢失</span></p><p id="u0ee679f7" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--log-dir						</span><span class="ne-text" style="color: #DF2A3F">//日志文件的路径</span></p><p id="u3d1afc4e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">Restart						</span><span class="ne-text" style="color: #DF2A3F">//定义了服务在失败时是否重启</span></p><p id="uf9b7346e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">RestartSec					</span><span class="ne-text" style="color: #DF2A3F">//定义了服务失败后重启前的等待时间</span></p></details>

同步文件到本机

```shell
cp kubelet-bootstrap.kubeconfig /etc/kubernetes/
cp kubelet.json /etc/kubernetes/
cp kubelet.service /usr/lib/systemd/system/
```

铜鼓文件到其他节点

```shell
for k8s in  master02 master03 node01 node02
do
	scp kubelet-bootstrap.kubeconfig kubelet.json $k8s:/etc/kubernetes/
	scp ca.pem $k8s:/etc/kubernetes/ssl/
	scp kubelet.service $k8s:/usr/lib/systemd/system/
done
```

master02节点修改`kubelet.json`中`address`为本机IP。

```shell
#...
"address": "192.168.0.112",
```

master03节点修改`kubelet.json`中`address`为本机IP。

```shell
#...
"address": "192.168.0.113",
```

node01节点修改`kubelet.json`中`address`为本机IP。

```shell
#...
"address": "192.168.0.114",
```

node02节点修改`kubelet.json`中`address`为本机IP。

```shell
#...
"address": "192.168.0.115",
```

所有节点创建kubelet工作目录

```shell
mkdir /var/lib/kubelet
```

## 启动kubelet服务

```shell
systemctl daemon-reload
systemctl start kubelet
systemctl enable kubelet
systemctl status kubelet
```

查看集群节点信息（在master节点执行命令）

```shell
kubectl get nodes
===========================================================
NAME       STATUS     ROLES    AGE   VERSION
master01   NotReady   <none>   11s   v1.23.0
master02   NotReady   <none>   13s   v1.23.0
master03   NotReady   <none>   15s   v1.23.0
node01     NotReady   <none>   16s   v1.23.0
node02     NotReady   <none>   19s   v1.23.0
```

# 第九章：部署Kube-Proxy

## 生成kube-proxy相关证书

生成kube-proxy证书签名请求文件

```shell
cat > kube-proxy-csr.json << "EOF"
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "yesir",
      "OU": "CN"
    }
  ]
}
EOF
```

使用现有的CA证书及其私钥 ，对`kube-proxy` 证书签名请求文件进行签署，生成一个新的https证书和对应的私钥

```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
```

会生成如下证书与之配套的文件：

-  `kube-proxy.csr`			//证书配套的签名请求文件, 包含证书中定义的一些信息 
-  `kube-proxy-key.pem`   //证书配套的私钥文件, 验证证书的真实性 
-  `kube-proxy.pem`       	//证书文件 

## 配置kube-proxy上下文

在`kube-proxy.kubeconfig`文件中指定集群名称、集群地址及端口

```shell
kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=https://192.168.0.100:6443 --kubeconfig=kube-proxy.kubeconfig
```

在`kube-proxy.kubeconfig`文件中添加`kube-proxy`用户

```shell
kubectl config set-credentials kube-proxy --client-certificate=kube-proxy.pem --client-key=kube-proxy-key.pem --embed-certs=true --kubeconfig=kube-proxy.kubeconfig
```

创建kube-proxy上下文信息

```shell
kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig
```

通过文件切换到集群验证

```shell
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

## 创建kube-proxy服务配置文件

定义了 Kube-proxy 运行所需的一些配置参数

```shell
cat > kube-proxy.yaml << "EOF"
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 192.168.0.111
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: 10.244.0.0/16
healthzBindAddress: 192.168.0.111:10256
kind: KubeProxyConfiguration
metricsBindAddress: 192.168.0.111:10249
mode: "ipvs"
EOF
```

<details class="lake-collapse"><summary id="ua359ef41"><strong><span class="ne-text">参数说明：</span></strong></summary><p id="ufd26f7b5" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">apiVersion				</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//表明此配置遵循的API版本</span></p><p id="ufbf99281" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">bindAddress				</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//kube-proxy监听的IP地址，用于接收传入的请求</span></p><p id="u1f22d9e1" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">clusterCIDR				</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//集群的IP地址范围，这样kube-proxy进行适当的路由</span></p><p id="u4b93ab45" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">healthzBindAddress		</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//kube-proxy健康检查端口，其他组件通过这个地址来检查它的状态</span></p><p id="udcbec81c" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">kind						</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//表明这个文件的类型是KubeProxyConfiguration</span></p><p id="ue4da0ca6" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">metricsBindAddress		</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//指定kube-proxy提供指标数据端口,在这个端口获取kube-proxy的性能指标</span></p><p id="u22747f58" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 14px">mode					</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//指定kube-proxy使用的代理模式为ipvs模式是一种基于内核的负载均衡技术</span></p></details>

## 创建kube-proxy服务管理文件

```shell
cat >  kube-proxy.service << "EOF"
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yaml \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```

同步本机

```shell
cp kube-proxy*.pem /etc/kubernetes/ssl/
cp kube-proxy.kubeconfig kube-proxy.yaml /etc/kubernetes/
cp kube-proxy.service /usr/lib/systemd/system/
```

其他节点

```shell
for k8s in master02 master03 node01 node02
do
  scp kube-proxy*.pem $k8s:/etc/kubernetes/ssl/
	scp kube-proxy.kubeconfig kube-proxy.yaml $k8s:/etc/kubernetes/
	scp kube-proxy.service $k8s:/usr/lib/systemd/system/
done
```



其他节点需要修改`kube-proxy.yaml` 文件中 `bindAddress、healthzBindAddress、metricsBindAddress` 为当前主机IP。

master02节点

```shell
#...
bindAddress: 192.168.0.112
healthzBindAddress: 192.168.0.112:10256
metricsBindAddress: 192.168.0.112:10249
```

master03节点

```shell
#...
bindAddress: 192.168.0.113
healthzBindAddress: 192.168.0.113:10256
metricsBindAddress: 192.168.0.113:10249
```

node01节点

```shell
#...
bindAddress: 192.168.0.114
healthzBindAddress: 192.168.0.114:10256
metricsBindAddress: 192.168.0.114:10249
```

node02节点

```shell
#...
bindAddress: 192.168.0.115
healthzBindAddress: 192.168.0.115:10256
metricsBindAddress: 192.168.0.115:10249
```

所有节点创建kube-proxy工作目录

```shell
mkdir /var/lib/kube-proxy
```

## 启动kube-proxy服务

```shell
systemctl daemon-reload
systemctl start kube-proxy
systemctl enable kube-proxy
systemctl status kube-proxy
```



# 第十章：部署集群网络与DNS

## 部署Calico网络

Calico 和 Flannel 是两种流行的 k8s 网络插件，它们都为集群中的 Pod 提供网络功能。然而，它们在实现方式和功能上有一些重要区别：

**网络模型的区别：**

**Calico** 使用 BGP（边界网关协议）作为其底层网络模型。它利用 BGP 为每个 Pod 分配一个唯一的 IP 地址，并在集群内部进行路由。Calico 支持网络策略，可以对流量进行精细控制，允许或拒绝特定的通信。

**Flannel** 则采用了一个简化的覆盖网络模型。它为每个节点分配一个 IP 地址子网，然后在这些子网之间建立覆盖网络。Flannel 将 Pod 的数据包封装到一个更大的网络数据包中，并在节点之间进行转发。Flannel 更注重简单和易用性，不提供与 Calico 类似的网络策略功能。

**性能的区别：**

**Calico** 使用 BGP 进行路由，其性能通常优于 Flannel。Calico 可以实现直接的 Pod 到 Pod 通信，而无需在节点之间进行额外的封装和解封装操作。这使得 Calico 在大型或高度动态的集群中具有更好的性能。

**Flannel** 的覆盖网络模型会导致额外的封装和解封装开销，从而影响网络性能。对于较小的集群或对性能要求不高的场景，这可能并不是一个严重的问题。



在master01节点下载calico文件

```shell
wget https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml
```

创建calico网络

```shell
kubectl apply -f calico.yaml 
```

查看calico的Pod状态是否为Running

```shell
kubectl get pod -n kube-system
===========================================================
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-66966888c4-whdkj   1/1     Running   0          101s
calico-node-f4ghp                          1/1     Running   0          101s
calico-node-sj88q                          1/1     Running   0          101s
calico-node-vnj7f                          1/1     Running   0          101s
calico-node-vwnw4                          1/1     Running   0          101s
```

查看集群节点的状态是否为Ready

```shell
kubectl get nodes
===========================================================
NAME       STATUS   ROLES    AGE     VERSION
master01   Ready    <none>   4h23m   v1.23.0
master02   Ready    <none>   4h22m   v1.23.0
master03   Ready    <none>   4h21m   v1.23.0
node01     Ready    <none>   4h21m   v1.23.0
node01     Ready    <none>   4h21m   v1.23.0
```

## 部署CoreDNS

在k8s中，很多功能都需要用到DNS服务，例如：服务发现、负载均衡、有状态应用的访问等。

<details class="lake-collapse"><summary id="u679d45fa"><strong><span class="ne-text" style="font-size: 22px">下载地址：</span></strong></summary><p id="u1e25fd0c" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><a href="https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed" data-href="https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed" target="_blank" class="ne-link"><span class="ne-text">https://github.com/coredns/deployment/blob/master/kubernetes/coredns.yaml.sed</span></a></p></details>

```shell
cat >  coredns.yaml << "EOF"
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
  - apiGroups:
    - ""
    resources:
    - endpoints
    - services
    - pods
    - namespaces
    verbs:
    - list
    - watch
  - apiGroups:
    - discovery.k8s.io
    resources:
    - endpointslices
    verbs:
    - list
    - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
          lameduck 5s
        }
        ready
        kubernetes cluster.local  in-addr.arpa ip6.arpa {
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf {
          max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/name: "CoreDNS"
spec:
  # replicas: not specified here:
  # 1. Default is 1.
  # 2. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      nodeSelector:
        kubernetes.io/os: linux
      affinity:
         podAntiAffinity:
           preferredDuringSchedulingIgnoredDuringExecution:
           - weight: 100
             podAffinityTerm:
               labelSelector:
                 matchExpressions:
                   - key: k8s-app
                     operator: In
                     values: ["kube-dns"]
               topologyKey: kubernetes.io/hostname
      containers:
      - name: coredns
        image: coredns/coredns:1.8.4
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.96.0.2
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP
EOF
```

创建CoreDNS

```shell
kubectl apply -f coredns.yaml
```

查看CoreDNS的Pod状态是否为Running

```shell
kubectl get pod -n kube-system
===========================================================
NAME                                       READY   STATUS
coredns-764bcc6b75-qmszv                   1/1     Running
```

## Master节点设置污点

如果master节点只作为集群的管理节点，可以对master节点设置污点，来禁止Pod调度到master节点。

设置污点类型`NoSchedule`

```shell
kubectl taint node master01 master02 master03 master:NoSchedule
```

查看污点信息

```shell
kubectl describe node master01 master02 master03 | grep -i taint
```

## 验证集群可用性

部署一个nginx到集群，并实现外部访问

```shell
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.20.2
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30002
```

创建Pod

```shell
kubectl apply -f nginx-test.yml
```

查看Pod

```shell
kubectl get pod
===========================================================
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          30s
```

查看Service代理

```shell
kubectl get svc
===========================================================
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        22h
nginx-svc    NodePort    10.96.72.136   <none>        80:30002/TCP   63s
```



浏览器访问集群任意节点IP:30002端口测试。

# 第十一章：Node节点加入集群

后续如需往集群中添加新的工作节点，可以通过以下方式添加。

## 节点前期环境准备

| 主机名        | IP地址 | 操作系统  | 主机角色 | 硬件配置  |
| ------------- | ------ | --------- | -------- | --------- |
| 192.168.0.116 | node03 | Rocky 9.0 | 工作节点 | 2C/4G/20G |

## 配置集群地址解析

```shell
cat >> /etc/hosts << EOF
192.168.0.111 master01
192.168.0.112 master02
192.168.0.113 master03
192.168.0.114 node01
192.168.0.115 node02
192.168.0.116 node03
EOF
```

集群现有节点增加node03地址解析

```shell
echo "192.168.0.116 node03" >> /etc/hosts
```

## **开启bridge网桥过滤**

bridge(桥接) 是 Linux 系统中的一种虚拟网络设备，它充当一个虚拟的交换机，为集群内的容器提供网络通信功能，容器就可以通过这个 bridge 与其他容器或外部网络通信了。

`/etc/sysctl.d/`目录用于存放配置内核参数的文件，文件以`.conf`结尾，这些文件会在系统启动时自动加载。

```shell
cat > /etc/sysctl.d/k8s.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
```

<details class="lake-collapse"><summary id="uc9a4f5f2"><strong><span class="ne-text">参数解释：</span></strong></summary><p id="u14ca5aa0" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="font-size: 16px">ne</span><span class="ne-text">t.bridge.bridge-nf-call-ip6tables = 1</span><strong><span class="ne-text"> </span></strong><span class="ne-text" style="background-color: #FBE4E7"> //对网桥上的IPv6数据包通过iptables处理</span></p><p id="u8952f810" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">net.bridge.bridge-nf-call-iptables = 1</span><strong><span class="ne-text">  </span></strong><span class="ne-text" style="background-color: #FBE4E7"> //对网桥上的IPv4数据包通过iptables处理</span></p><p id="u72413c7a" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">net.ipv4.ip_forward = 1  </span><strong><span class="ne-text">                      </span></strong><span class="ne-text" style="background-color: #FBE4E7"> //开启IPv4路由转发,来实现集群中的容器跨网络通信</span></p></details>

由于开启bridge功能，需要加载br_netfilter模块来允许在bridge设备上的数据包经过iptables防火墙处理

```shell
modprobe br_netfilter && lsmod | grep br_netfilter
```

<details class="lake-collapse"><summary id="uea136b84"><strong><span class="ne-text">命令解释：</span></strong></summary><p id="u4e436b21" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text" style="font-size: 24px">modprobe       	</span></strong><span class="ne-text" style="background-color: #FBE4E7"> //命令可以加载内核模块</span></p><p id="u9792f22c" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text" style="font-size: 24px">br_netfilter    		</span></strong><span class="ne-text" style="background-color: #FBE4E7">//模块模块允许在bridge设备上的数据包经过iptables防火墙处理</span></p></details>

加载配置文件，使上述配置生效

```shell
sysctl -p /etc/sysctl.d/k8s.conf
```

## **安装IPvs代理模块**

在k8s中Service有两种代理模式，一种是基于iptables的，一种是基于ipvs，两者对比ipvs模式性能更高效，如果想要使用ipvs模式，需要手动载入ipvs模块。

`ipset`和`ipvsadm`是网络管理和负载均衡相关的软件包，提供`ip_vs`模块

```shell
dnf -y install ipset ipvsadm
```

将需要加载的ipvs相关模块写入到文件中

`/etc/modules-load.d/`目录主要用于在系统启动时加载用户自定义的内核模块，这个目录中的文件以 `.conf` 结尾，文件中指定需要加载的内核模块。

```shell
cat > /etc/modules-load.d/ip_vs.conf <<EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
EOF
```

<details class="lake-collapse"><summary id="u3bba9512"><strong><span class="ne-text">模块介绍：</span></strong></summary><p id="u7af76083" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text" style="font-size: 24px">ip_vs         	</span></strong><span class="ne-text" style="background-color: #FBE4E7"> //提供负载均衡的模块,支持多种负载均衡算法,如轮询、最小连接、加权最小连接等</span></p><p id="ub87aa422" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text" style="font-size: 24px">ip_vs_rr      	 </span></strong><span class="ne-text" style="background-color: #FBE4E7">//轮询算法的模块（默认算法）</span></p><p id="u2baaa4aa" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text" style="font-size: 24px">ip_vs_wrr     	</span></strong><span class="ne-text" style="background-color: #FBE4E7"> //加权轮询算法的模块,根据后端服务器的权重值转发请求</span></p><p id="ua2e7f3a1" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text" style="font-size: 24px">ip_vs_sh     	</span></strong><span class="ne-text" style="background-color: #FBE4E7"> //哈希算法的模块,同一客户端的请求始终被分发到相同的后端服务器,保证会话一致性</span></p><p id="u5666a02e" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><strong><span class="ne-text" style="font-size: 24px">nf_conntrack  	</span></strong><span class="ne-text" style="color: #DF2A3F"> </span><span class="ne-text" style="background-color: #FBE4E7">//链接跟踪的模块,用于跟踪一个连接的状态,例如 TCP 握手、数据传输和连接关闭等</span></p></details>

 加载模块生效（这个文件会在下次系统启动时自动生效）

```shell
systemctl restart systemd-modules-load.service
```

过滤模块，验证是否成功加载  

```shell
lsmod | grep ip_vs
===========================================================
ip_vs_sh               16384  0
ip_vs_wrr              16384  0
ip_vs_rr               16384  0
ip_vs                 188416  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr
nf_conntrack          176128  1 ip_vs
nf_defrag_ipv6         24576  2 nf_conntrack,ip_vs
libcrc32c              16384  3 nf_conntrack,xfs,ip_vs
```

## **关闭SWAP分区**

为了保证 kubelet 正常工作，k8s强制要求禁用

```shell
swapoff -a && \
sed -ri 's/.*swap.*/#&/' /etc/fstab
```

## **Docker环境准备**

添加阿里云docker-ce仓库

```shell
dnf install -y yum-utils && \
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
```

安装docker软件包

```shell
dnf install docker-ce-20.10.24-3.el9.x86_64 -y
```

启用Docker Cgroup用于限制进程的资源使用量，如CPU、内存资源

```shell
# step 1：创建/etc/docker目录
mkdir /etc/docker
# step 2：配置Cgroup和镜像加速器
cat > /etc/docker/daemon.json <<EOF
{
        "exec-opts": ["native.cgroupdriver=systemd"],
        "registry-mirrors": ["https://docker.rainbond.cc"]
}
EOF
```

启动docker并设置随机自启

```shell
systemctl enable docker --now
```

## 部署kubelet组件

在master01节点拷贝公钥到node03节点

```shell
ssh-copy-id node03
```

在master01节点拷贝`kubelet、kube-proxy`程序到node03

```shell
cd /usr/local/bin/
scp kubelet kube-proxy node03:/usr/local/bin
```

在node03节点准备相关目录

```shell
#配置文件目录
mkdir -p /etc/kubernetes
#证书目录
mkdir -p /etc/kubernetes/ssl
#kubelet目录
mkdir -p /var/lib/kubelet
#日志目录
mkdir -p /var/log/kubernetes
```

在master01节点拷贝`kubelet.json、kubelet-bootstrap.kubeconfig、kubelet.kubeconfig、kubelet.service、ca.pem` 文件到node03节点

```shell
scp kubelet-bootstrap.kubeconfig kubelet.json node03:/etc/kubernetes/
scp /etc/kubernetes/kubelet.kubeconfig node03:/etc/kubernetes/
scp kubelet.service node03:/usr/lib/systemd/system
scp ca.pem node03:/etc/kubernetes/ssl
```

在node03节点需要修改`kubelet.json`文件，修改其中的IP地址为本机

```shell
#...
"address": "192.168.0.116",
```

## 启动kubelet

```shell
systemctl daemon-reload
systemctl start kubelet
systemctl enable kubelet
systemctl status kubelet
```

在master01查看节点信息

```shell
kubectl get node
===========================================================
NAME          STATUS     ROLES    AGE    VERSION
master01   		Ready      <none>   2d2h   v1.23.0
master02   		Ready      <none>   2d2h   v1.23.0
master03   		Ready      <none>   2d2h   v1.23.0
node01     		Ready      <none>   2d2h   v1.23.0
node02     		Ready   	 <none>   2d2h   v1.23.0
node03     		NotReady   <none>   28s    v1.23.0
```

## 部署kube-proxy组件

同步kube-proxy相关文件到节node03点

```shell
scp kube-proxy*.pem node03:/etc/kubernetes/ssl/
scp kube-proxy.kubeconfig kube-proxy.yaml node03:/etc/kubernetes/
scp kube-proxy.service node03:/usr/lib/systemd/system/
```

将`kube-proxy.yaml` 文件中 `bindAddress、healthzBindAddress、metricsBindAddress` 需要修改为当前主机IP地址。

```shell
#...
bindAddress: 192.168.0.116
healthzBindAddress: 192.168.0.116:10256
metricsBindAddress: 192.168.0.116:10249
```

创建kube-proxy工作目录

```shell
mkdir /var/lib/kube-proxy
```

启动kube-proxy服务

```shell
systemctl daemon-reload
systemctl start kube-proxy
systemctl enable kube-proxy
systemctl status kube-proxy
```

在master01查看节点信息

```shell
kubectl get node
===========================================================
NAME          STATUS     ROLES    AGE    VERSION
master01   		Ready      <none>   2d2h   v1.23.0
master02   		Ready      <none>   2d2h   v1.23.0
master03   		Ready      <none>   2d2h   v1.23.0
node01     		Ready      <none>   2d2h   v1.23.0
node02     		Ready   	 <none>   2d2h   v1.23.0
node03     		Ready   	 <none>   2m     v1.23.0
```



# 第十二章：集群删除节点流程

1. 安全驱逐节点上的所有Pods

```shell
kubectl drain node03 --delete-emptydir-data --force --ignore-daemonsets
```

<details class="lake-collapse"><summary id="u5f12e325"><strong><span class="ne-text" style="font-size: 22px">命令介绍：</span></strong></summary><p id="u67f82224" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="color: rgb(51, 51, 51); font-size: 14px">--delete-emptydir-data		</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//在驱逐Pods时删除使用EmptyDir卷的任何数据。</span></p><p id="u98255212" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text">--force</span><span class="ne-text" style="color: rgb(51, 51, 51); font-size: 14px">					</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//强制删除该节点运行的Pod。</span></p><p id="u56db8489" class="ne-p" style="margin: 0; padding: 0; min-height: 24px"><span class="ne-text" style="color: rgb(51, 51, 51); font-size: 14px">--ignore-daemonsets		</span><span class="ne-text" style="color: #DF2A3F; font-size: 14px">//忽视DaemonSet管理的Pod。</span></p></details>

查看节点状态

```shell
kubectl get node
===========================================================
NAME          STATUS     								ROLES    AGE    VERSION
master01   		Ready      								<none>   2d2h   v1.23.0
master02   		Ready      								<none>   2d2h   v1.23.0
master03   		Ready      								<none>   2d2h   v1.23.0
node01     		Ready      								<none>   2d2h   v1.23.0
node02     		Ready   	 								<none>   2d2h   v1.23.0
node03     		Ready,SchedulingDisabled  <none>   3m     v1.23.0
```

1. 从集群中彻底删除节点。

```shell
kubectl delete node node03
```

查看节点数量

```shell
kubectl get node
===========================================================
NAME          STATUS     ROLES    AGE    VERSION
master01   		Ready      <none>   2d2h   v1.23.0
master02   		Ready      <none>   2d2h   v1.23.0
master03   		Ready      <none>   2d2h   v1.23.0
node01     		Ready      <none>   2d2h   v1.23.0
node02     		Ready   	 <none>   2d2h   v1.23.0
```



**提示：**node03重启后，由于设置了kubelet随机自启，所以还会再次自动加入集群，如果不希望自动加入集群，需取消kubelet的随机自启。

```shell
systemctl disable kubelet
```

# ingress-nginx

```bash
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.6.4/deploy/static/provider/baremetal/deploy.yaml
```

~~~~yaml
apiVersion: v1
kind: Namespace
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  name: ingress-nginx
---
apiVersion: v1
automountServiceAccountToken: true
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx
  namespace: ingress-nginx
rules:
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - configmaps
  - pods
  - secrets
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - coordination.k8s.io
  resourceNames:
  - ingress-nginx-leader
  resources:
  - leases
  verbs:
  - get
  - update
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - list
  - watch
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-admission
  namespace: ingress-nginx
rules:
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
  - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - endpoints
  - nodes
  - pods
  - secrets
  - namespaces
  verbs:
  - list
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses/status
  verbs:
  - update
- apiGroups:
  - networking.k8s.io
  resources:
  - ingressclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - list
  - watch
  - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-admission
rules:
- apiGroups:
  - admissionregistration.k8s.io
  resources:
  - validatingwebhookconfigurations
  verbs:
  - get
  - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx
subjects:
- kind: ServiceAccount
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-admission
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx-admission
subjects:
- kind: ServiceAccount
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx
subjects:
- kind: ServiceAccount
  name: ingress-nginx
  namespace: ingress-nginx
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-admission
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx-admission
subjects:
- kind: ServiceAccount
  name: ingress-nginx-admission
  namespace: ingress-nginx
---
apiVersion: v1
data:
  allow-snippet-annotations: "true"
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-controller
  namespace: ingress-nginx
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - appProtocol: http
    name: http
    port: 80
    protocol: TCP
    targetPort: http
  - appProtocol: https
    name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-controller-admission
  namespace: ingress-nginx
spec:
  ports:
  - appProtocol: https
    name: https-webhook
    port: 443
    targetPort: webhook
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --election-id=ingress-nginx-leader
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.6.3@sha256:b92667e0afde1103b736e6a3f00dd75ae66eec4e71827d19f19f471699e909d2
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-admission-create
  namespace: ingress-nginx
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: admission-webhook
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.6.3
      name: ingress-nginx-admission-create
    spec:
      containers:
      - args:
        - create
        - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc
        - --namespace=$(POD_NAMESPACE)
        - --secret-name=ingress-nginx-admission
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343@sha256:39c5b2e3310dc4264d638ad28d9d1d96c4cbb2b2dcfb52368fe4e3c63f61e10f
        imagePullPolicy: IfNotPresent
        name: create
        securityContext:
          allowPrivilegeEscalation: false
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      securityContext:
        fsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
      serviceAccountName: ingress-nginx-admission
---
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-admission-patch
  namespace: ingress-nginx
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: admission-webhook
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.6.3
      name: ingress-nginx-admission-patch
    spec:
      containers:
      - args:
        - patch
        - --webhook-name=ingress-nginx-admission
        - --namespace=$(POD_NAMESPACE)
        - --patch-mutating=false
        - --secret-name=ingress-nginx-admission
        - --patch-failure-policy=Fail
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343@sha256:39c5b2e3310dc4264d638ad28d9d1d96c4cbb2b2dcfb52368fe4e3c63f61e10f
        imagePullPolicy: IfNotPresent
        name: patch
        securityContext:
          allowPrivilegeEscalation: false
      nodeSelector:
        kubernetes.io/os: linux
      restartPolicy: OnFailure
      securityContext:
        fsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
      serviceAccountName: ingress-nginx-admission
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: nginx
spec:
  controller: k8s.io/ingress-nginx
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  labels:
    app.kubernetes.io/component: admission-webhook
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.6.3
  name: ingress-nginx-admission
webhooks:
- admissionReviewVersions:
  - v1
  clientConfig:
    service:
      name: ingress-nginx-controller-admission
      namespace: ingress-nginx
      path: /networking/v1/ingresses
  failurePolicy: Fail
  matchPolicy: Equivalent
  name: validate.nginx.ingress.kubernetes.io
  rules:
  - apiGroups:
    - networking.k8s.io
    apiVersions:
    - v1
    operations:
    - CREATE
    - UPDATE
    resources:
    - ingresses
  sideEffects: None
~~~~